{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SamuelRuby/DeepLearningProjects/blob/main/VAE_for_Molecular_Representation_AND_Generation_ZINC_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTO0YPkruTg2"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade numpy\n",
        "!pip install --upgrade scipy\n",
        "!pip install --upgrade scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbTlp1CZC8Px"
      },
      "outputs": [],
      "source": [
        "!pip install rdkit-pypi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VR3du0wwx5Tr"
      },
      "outputs": [],
      "source": [
        "# ------------------                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          # --------------------\n",
        "# ---LIBRARIES------\n",
        "# ------------------\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "# from sklearn.datasets import fetch_openaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-Rf4xnbtoVI"
      },
      "outputs": [],
      "source": [
        "from rdkit import Chem, DataStructs\n",
        "from rdkit.Chem import Draw, AllChem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rI7-J9PeOisv"
      },
      "source": [
        "# ** PHASE 1: Setup & Data Exploration **"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Oh1YrFXOr_j"
      },
      "source": [
        "## Step 1.1: Understand the ZINC dataset structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tMTocQeLpCv"
      },
      "outputs": [],
      "source": [
        "# Find and load a subset of the ZINC dataset (SMILES format is standard).\n",
        "\n",
        "# Understand what SMILES strings are (how they represent molecules).\n",
        "\n",
        "# Visualize a few molecules from their SMILES strings.\n",
        "\n",
        "# Optional: convert SMILES to molecular fingerprints (to understand structure)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-16Xkay3NRV5"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/schwallergroup/ai4chem_course/generative_models/notebooks/05%20-%20Generative%20Models/data/zinc.smi -O zinc.smi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0rm0YvjyGCx"
      },
      "outputs": [],
      "source": [
        "# Load SMILES strings\n",
        "zinc_data = pd.read_csv('zinc.smi', header=None, skiprows=1)\n",
        "zinc_data.columns = ['SMILES']\n",
        "print(zinc_data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLHpLeG3NpXu"
      },
      "outputs": [],
      "source": [
        "# VISUALIZE\n",
        "# Convert SMILES to RDKit molecule objects\n",
        "molecules = [Chem.MolFromSmiles(smiles) for smiles in zinc_data['SMILES'][:5]]\n",
        "\n",
        "# Filter out None values from the molecules list\n",
        "molecules = [mol for mol in molecules if mol is not None]\n",
        "\n",
        "# Visualize molecules\n",
        "Draw.MolsToGridImage(molecules, molsPerRow=3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDYv5iZeCIyb"
      },
      "outputs": [],
      "source": [
        "# #visulaize\n",
        "# #zinc_data['SMILES']\n",
        "# smiles_list = zinc_data['SMILES'].tolist()[:5] # Get the first 5 SMILES strings\n",
        "# mols = [Chem.MolFromSmiles(smiles) for smiles in smiles_list] # Convert SMILES to RDKit Mol objects\n",
        "# Draw.MolsToGridImage(mols, molsPerRow=5, subImgSize=(200, 200), legends=smiles_list) # Display molecules in a grid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8tvYHH_ProR"
      },
      "outputs": [],
      "source": [
        "#converting smiles to molecular fingerprints\n",
        "\n",
        "# Generate fingerprints, but only if molecules is not empty\n",
        "if molecules:\n",
        "    fingerprints = [AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=1024) for mol in molecules]\n",
        "\n",
        "    # Example: Print the fingerprint of the first molecule\n",
        "    print(fingerprints[0])\n",
        "else:\n",
        "    print(\"No valid molecules found. Skipping fingerprint generation.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLScJOo6Qaw6"
      },
      "outputs": [],
      "source": [
        "#print(fingerprints[0].ToBitString())\n",
        "\n",
        "fingerprint_array = np.array(fingerprints[0])\n",
        "plt.imshow(fingerprint_array.reshape(32, 32), cmap='viridis')  # Adjust shape as needed\n",
        "plt.colorbar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtuPQtQKQdMo"
      },
      "outputs": [],
      "source": [
        "from rdkit import DataStructs\n",
        "\n",
        "similarity = DataStructs.FingerprintSimilarity(fingerprints[0], fingerprints[1])  # Compare the first two fingerprints\n",
        "print(similarity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQkKuXTQQiRp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcPccsorjJqv"
      },
      "source": [
        "## Step 1.2: Preprocess the Data for the VAE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmCFGXi1i064"
      },
      "source": [
        "Tasks:\n",
        "\n",
        "Tokenize SMILES strings (each character or token becomes part of your vocabulary).\n",
        "\n",
        "Convert SMILES to sequences of tokens (integer encoding).\n",
        "\n",
        "Pad sequences to the same length.\n",
        "\n",
        "Split into train/validation/test sets.\n",
        "\n",
        "Goal: Prepare data for feeding into your VAE. Understand that each molecule becomes a sequence of tokens.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lk_ddnZbl5Ym"
      },
      "outputs": [],
      "source": [
        "# tokenize\n",
        "# def tokenize_smiles(smiles):\n",
        "#     tokens = list(smiles)  # Each character becomes a token\n",
        "#     return tokens\n",
        "\n",
        "# tokenized_smiles = [tokenize_smiles(smiles) for smiles in zinc_data['SMILES']]\n",
        "# print(tokenized_smiles)\n",
        "\n",
        "# Tokens represent atoms, bonds, and structural features, enabling computational analysis\n",
        "# Tokenized SMILES strings can be fed into NLP models for tasks like molecular property prediction or generative modeling."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Try this more sophisticated tokenization approach\n",
        "def tokenize_smiles(smiles):\n",
        "    \"\"\"Tokenize SMILES with awareness of chemical structures\"\"\"\n",
        "    import re\n",
        "    pattern = r'(\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\|\\/|:|~|@|\\?|>|\\*|\\$|\\%[0-9]{2}|[0-9])'\n",
        "    regex = re.compile(pattern)\n",
        "    tokens = [token for token in regex.findall(smiles)]\n",
        "    return tokens\n",
        "tokenized_smiles = [tokenize_smiles(smiles) for smiles in zinc_data['SMILES']]\n",
        "print(tokenized_smiles)"
      ],
      "metadata": {
        "id": "adn8FXXmmh3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HuNUKKAxjVYv"
      },
      "outputs": [],
      "source": [
        "# Vocab : Maps each unique token to an integer, creating a consistent encoding scheme.\n",
        "def build_vocabulary(tokenizedd_smiles):\n",
        "    all_tokens = set()\n",
        "    for smiles in tokenizedd_smiles:\n",
        "        all_tokens.update(smiles)\n",
        "    vocab = {token: idx for idx, token in enumerate(sorted(all_tokens))}\n",
        "    return vocab\n",
        "\n",
        "vocab = build_vocabulary(tokenized_smiles)\n",
        "print(vocab)  # Displays the token-to-integer mapping\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3j6TYoI3pHbO"
      },
      "outputs": [],
      "source": [
        "# integer encoding SMILES Strings: Transforms SMILES strings into sequences of integers, used as input for the machine learning model\n",
        "def encode_smiles(smiles, vocab):\n",
        "    tokens = tokenize_smiles(smiles)\n",
        "    encoded = [vocab[token] for token in tokens]\n",
        "    return encoded\n",
        "\n",
        "encoded_smiles = [encode_smiles(smiles, vocab) for smiles in zinc_data['SMILES']]\n",
        "print(encoded_smiles[:5])  # Displays the integer-encoded sequences for the first 5 SMILES strings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYB7VJduskim"
      },
      "outputs": [],
      "source": [
        "#padding sequences to same length\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Set maximum sequence length\n",
        "max_len = max(len(seq) for seq in encoded_smiles)\n",
        "\n",
        "# Pad sequences with zeros\n",
        "padded_smiles = pad_sequences(encoded_smiles, maxlen=max_len, padding='post', value=0)\n",
        "print(padded_smiles[:5])  # Displays the first 5 padded sequences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6-16T-ZtU4r"
      },
      "outputs": [],
      "source": [
        "#split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split into training and test sets (80% train, 20% test)\n",
        "train_smiles, test_smiles = train_test_split(padded_smiles, test_size=0.2, random_state=42)\n",
        "\n",
        "# Further split training set into training and validation sets (80% train, 20% validation)\n",
        "train_smiles, val_smiles = train_test_split(train_smiles, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "#added here\n",
        "# Reduce each set by half\n",
        "train_smiles = train_smiles[:len(train_smiles) // 2]\n",
        "val_smiles = val_smiles[:len(val_smiles) // 2]\n",
        "test_smiles = test_smiles[:len(test_smiles) // 2]\n",
        "\n",
        "\n",
        "print(f\"Training set size: {len(train_smiles)}\")\n",
        "print(f\"Validation set size: {len(val_smiles)}\")\n",
        "print(f\"Test set size: {len(test_smiles)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNY7zBiRth1-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbfL8ShFvS-F"
      },
      "source": [
        "# ** PHASE 2: Build the VAE Architecture **"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHRj1nqGvBNP"
      },
      "source": [
        "## Step 2.1: Implement the Encoder\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aW-xM-o7vkv_"
      },
      "source": [
        "Tasks:\n",
        "\n",
        "Define an embedding layer (to map tokens to dense vectors).\n",
        "\n",
        "Use an LSTM or GRU to encode the sequence.\n",
        "\n",
        "Extract the mean and log variance (for the latent space).\n",
        "\n",
        "Goal: Understand how the model compresses a molecule into a latent vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxw0V9y-1pFY"
      },
      "outputs": [],
      "source": [
        "# LIBRARIES\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import LSTM, GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKy__jl-fw7w"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmJBqlG6vdj_"
      },
      "outputs": [],
      "source": [
        "# define the embedding layer: that maps tokens (integer-encoded representations of SMILES strings) into dense vectors. therefore the model learn meaningful representations of individual tokens.\n",
        "\n",
        "vocab_size = len(vocab)  # Total number of unique tokens #size of the vocabulary\n",
        "embedding_dim = 128  # Dimension of the embedding space\n",
        "\n",
        "embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len)\n",
        "\n",
        "#  output_dim=embedding_dim -->defines the dimensionality of the dense embedding vectors. Higher dimensions capture more information but require more computational power.\n",
        "\n",
        "#input_length=max_len: The fixed sequence length after padding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JU4PCKLPgBUA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OextSHAG0Pl3"
      },
      "outputs": [],
      "source": [
        "#LSTM OR GRU to Encode the Sequence and extract patterns from the SMILES string\n",
        "\n",
        "# Both are capable of capturing sequential relationships in the data:\n",
        "# LSTM (Long Short-Term Memory): Ideal for handling long-term dependencies.\n",
        "# GRU (Gated Recurrent Unit): Similar to LSTM but computationally more efficient.\n",
        "\n",
        "\n",
        "# Define the encoder (LSTM or GRU) --> START with GRU\n",
        "latent_dim = 64  # Dimensionality of the latent vector #was 64 increased to 128\n",
        "\n",
        "# encoder_lstm = LSTM(latent_dim, return_sequences=False, return_state=True)\n",
        "# OR\n",
        "encoder_gru = GRU(latent_dim, return_sequences=False, return_state=True)\n",
        "\n",
        "# Pass padded integer-encoded sequences through encoder embedding layer\n",
        "inputs = np.array(padded_smiles) # Padded integer-encoded sequences\n",
        "embedded_sequences = embedding_layer(inputs) #padded tensor should be what is passed into embedding_layer function\n",
        "_, state_h = encoder_gru(embedded_sequences)\n",
        "\n",
        "# state_h is latent space\n",
        "# return_sequences=False: Only the final output of the sequence is returned (compressed information).\n",
        "# return_state=True: Captures the hidden state as well, which is critical for the latent representation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dETgY17cgWE_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6imd_Ql3rhP"
      },
      "outputs": [],
      "source": [
        "# Extract the mean and log variance (for the latent space). a probabilistic latent space\n",
        "\n",
        "z_mean = Dense(latent_dim)(state_h) #mu\n",
        "z_log_var = Dense(latent_dim)(state_h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sz16topEDPsb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61223hzPDQNi"
      },
      "source": [
        "## Step 2.2: Reparametization Technique --> Latent sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyPliRVh_opF"
      },
      "outputs": [],
      "source": [
        "# Sampling: Generates the latent vector using the reparameterization trick.\n",
        "\n",
        "def sampling(z_mean, z_log_var):\n",
        "    epsilon = tf.random.normal(shape=tf.shape(z_mean)) #ε is random noise sampled from a standard normal distribution.\n",
        "    z = z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "    return z\n",
        "\n",
        "# Sample latent vector\n",
        "z = sampling(z_mean, z_log_var)\n",
        "\n",
        "#this trick in VAE involves sampling from a normal distribution (one that has mean as 0 and std as 1), and then scaling/shifting our distribution using the latent variables(mean and std)\n",
        "# this allows gradients to flow(backpropagate) through the sampling operation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81xrkdeq_qHQ"
      },
      "outputs": [],
      "source": [
        "z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAbKDVBo_sBc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLP0eL38Hvz_"
      },
      "source": [
        "\n",
        "## Step 2.3: Implement the Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-14Lj4HVH5vl"
      },
      "outputs": [],
      "source": [
        "# Tasks:\n",
        "\n",
        "# Take latent vectors and decode them into sequences.\n",
        "\n",
        "# Use a recurrent decoder (LSTM/GRU) + a dense layer with softmax.\n",
        "\n",
        "# Train the model to reconstruct the original SMILES from latent space.\n",
        "\n",
        "# Goal: Learn how the VAE \"imagines\" or regenerates molecules.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFO--wJEPRHE"
      },
      "outputs": [],
      "source": [
        "# LIBARIES\n",
        "# from tensorflow.keras.layers import Input, RepeatVector\n",
        "# from tensorflow.keras.models import Model\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import Model, Input\n",
        "from tensorflow.keras.layers import Embedding, GRU, Dense, RepeatVector\n",
        "import numpy as np\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWgepSjhPP7C"
      },
      "outputs": [],
      "source": [
        "# BUILD DECODER\n",
        "# Decoder Input: Latent Vector decoding into sequences\n",
        "decoder_input = Input(shape=(latent_dim,))  # The latent vector size is `latent_dim`\n",
        "\n",
        "# Expand Latent Vector for Sequence Generation\n",
        "repeat_vector = RepeatVector(max_len)(decoder_input)  # Repeat to match sequence length\n",
        "\n",
        "# recurrent Decoder GRU/LSTM\n",
        "decoder_gru = GRU(256, return_sequences=True)(repeat_vector)  # 256 units, recurrent processing\n",
        "# OR\n",
        "# decoder_lstm = LSTM(256, return_sequences=True)(repeat_vector)\n",
        "\n",
        "# Dense Layer with Softmax\n",
        "decoder_dense = Dense(vocab_size, activation='softmax')(decoder_gru)\n",
        "\n",
        "# Define the decoder model\n",
        "decoder = Model(decoder_input, decoder_dense)\n",
        "\n",
        "#define loss function\n",
        "decoder.compile(optimizer='adam', loss='categorical_crossentropy')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ePZPd7DeZNZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ** PHASE 3: TRAINING **"
      ],
      "metadata": {
        "id": "ZUXLcgEVMs3p"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9T0nN5di9EC"
      },
      "source": [
        "\n",
        "## Step 3.1:  PUTTING EVERYTHING TOGETHER INTO A CLASS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awk4xbMsJ7oa"
      },
      "outputs": [],
      "source": [
        "# Encoder layers\n",
        "encoder_gru = GRU(64, return_sequences=False, return_state=True)\n",
        "z_mean_dense = Dense(64)\n",
        "z_log_var_dense = Dense(64)\n",
        "\n",
        "# Sampling layer\n",
        "def sampling(z_mean, z_log_var):\n",
        "    epsilon = tf.random.normal(shape=tf.shape(z_mean))\n",
        "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_cTxZ6Qi8Pb"
      },
      "outputs": [],
      "source": [
        "#OLDDD\n",
        "# Decoder (already defined)\n",
        "# decoder_input = Input(shape=(latent_dim,))\n",
        "# repeat_vector = RepeatVector(max_len)(decoder_input)\n",
        "# decoder_gru = GRU(256, return_sequences=True)(repeat_vector)\n",
        "# decoder_dense = Dense(vocab_size, activation='softmax')(decoder_gru)\n",
        "# decoder = Model(decoder_input, decoder_dense)\n",
        "\n",
        "# 🧠 VAE Class\n",
        "# class VAE(Model):\n",
        "#     def __init__(self, embedding_layer, encoder_gru, z_mean_dense, z_log_var_dense, decoder, **kwargs):\n",
        "#         super(VAE, self).__init__(**kwargs)\n",
        "#         self.embedding_layer = embedding_layer\n",
        "#         self.encoder_gru = encoder_gru\n",
        "#         self.z_mean_dense = z_mean_dense\n",
        "#         self.z_log_var_dense = z_log_var_dense\n",
        "#         self.decoder = decoder\n",
        "\n",
        "#     def train_step(self, data):\n",
        "#         x = data  # data = padded SMILES input (batch)\n",
        "#         with tf.GradientTape() as tape:\n",
        "#             # 1. Embed\n",
        "#             embedded = self.embedding_layer(x)\n",
        "\n",
        "#             # 2. Encode\n",
        "#             _, state_h = self.encoder_gru(embedded)\n",
        "#             z_mean = self.z_mean_dense(state_h)\n",
        "#             z_log_var = self.z_log_var_dense(state_h)\n",
        "#             z = sampling(z_mean, z_log_var)\n",
        "\n",
        "#             # 3. Decode\n",
        "#             x_reconstructed = self.decoder(z)\n",
        "\n",
        "#             # 4. Losses\n",
        "#             reconstruction_loss = tf.reduce_mean(\n",
        "#                 tf.keras.losses.sparse_categorical_crossentropy(x, x_reconstructed)\n",
        "#             )\n",
        "#             kl_loss = -0.5 * tf.reduce_mean(\n",
        "#                 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
        "#             )\n",
        "#             total_loss = reconstruction_loss + kl_loss\n",
        "\n",
        "#         # Backprop\n",
        "#         grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "#         self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "\n",
        "#         return {\n",
        "#             \"loss\": total_loss,\n",
        "#             \"reconstruction_loss\": reconstruction_loss,\n",
        "#             \"kl_loss\": kl_loss\n",
        "#         }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTre7gJlrGZm"
      },
      "outputs": [],
      "source": [
        "#changed code for implementing a call(method)\n",
        "class VAE(Model):\n",
        "    def __init__(self, embedding_layer, encoder_gru, z_mean_dense, z_log_var_dense, decoder, **kwargs):\n",
        "        super(VAE, self).__init__(**kwargs)\n",
        "        self.embedding_layer = embedding_layer\n",
        "        self.encoder_gru = encoder_gru\n",
        "        self.z_mean_dense = z_mean_dense\n",
        "        self.z_log_var_dense = z_log_var_dense\n",
        "        self.decoder = decoder\n",
        "\n",
        "        # Define metrics\n",
        "        self.total_loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
        "        self.reconstruction_loss_tracker = keras.metrics.Mean(name=\"reconstruction_loss\")\n",
        "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.total_loss_tracker,\n",
        "            self.reconstruction_loss_tracker,\n",
        "            self.kl_loss_tracker,\n",
        "        ]\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Forward pass for inference\n",
        "        embedded = self.embedding_layer(inputs)\n",
        "        _, state_h = self.encoder_gru(embedded)\n",
        "        z_mean = self.z_mean_dense(state_h)\n",
        "        z_log_var = self.z_log_var_dense(state_h)\n",
        "        z = sampling(z_mean, z_log_var)\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def train_step(self, data):\n",
        "        x = data  # data = padded SMILES input (batch)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            # 1. Embed\n",
        "            embedded = self.embedding_layer(x)\n",
        "\n",
        "            # 2. Encode\n",
        "            _, state_h = self.encoder_gru(embedded)\n",
        "            z_mean = self.z_mean_dense(state_h)\n",
        "            z_log_var = self.z_log_var_dense(state_h)\n",
        "            z = sampling(z_mean, z_log_var)\n",
        "\n",
        "            # 3. Decode\n",
        "            x_reconstructed = self.decoder(z)\n",
        "\n",
        "            # 4. Losses\n",
        "            reconstruction_loss = tf.reduce_mean(\n",
        "                tf.keras.losses.sparse_categorical_crossentropy(x, x_reconstructed)\n",
        "            )\n",
        "            kl_loss = -0.5 * tf.reduce_mean(\n",
        "                1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
        "            )\n",
        "            total_loss = reconstruction_loss + kl_loss\n",
        "\n",
        "        # Backprop\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "\n",
        "        # Update metrics\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "\n",
        "        return {\n",
        "            \"loss\": self.total_loss_tracker.result(),\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "            \"kl_loss\": self.kl_loss_tracker.result()\n",
        "        }\n",
        "\n",
        "    def test_step(self, data):\n",
        "        # This is needed for validation to work\n",
        "        x = data\n",
        "\n",
        "        # Forward pass\n",
        "        embedded = self.embedding_layer(x)\n",
        "        _, state_h = self.encoder_gru(embedded)\n",
        "        z_mean = self.z_mean_dense(state_h)\n",
        "        z_log_var = self.z_log_var_dense(state_h)\n",
        "        z = sampling(z_mean, z_log_var)\n",
        "        x_reconstructed = self.decoder(z)\n",
        "\n",
        "        # Compute losses\n",
        "        reconstruction_loss = tf.reduce_mean(\n",
        "            tf.keras.losses.sparse_categorical_crossentropy(x, x_reconstructed)\n",
        "        )\n",
        "        kl_loss = -0.5 * tf.reduce_mean(\n",
        "            1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
        "        )\n",
        "        total_loss = reconstruction_loss + kl_loss\n",
        "\n",
        "        # Update metrics\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "\n",
        "        return {\n",
        "            \"loss\": self.total_loss_tracker.result(),\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "            \"kl_loss\": self.kl_loss_tracker.result()\n",
        "        }\n",
        "#added get_config and from_config below\n",
        "    def get_config(self):\n",
        "      config = super(VAE, self).get_config()\n",
        "      config.update({\n",
        "          'embedding_layer': self.embedding_layer,\n",
        "          'encoder_gru': self.encoder_gru,\n",
        "          'z_mean_dense': self.z_mean_dense,\n",
        "          'z_log_var_dense': self.z_log_var_dense,\n",
        "          'decoder': self.decoder\n",
        "      })\n",
        "      return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zaw89toy_p8V"
      },
      "outputs": [],
      "source": [
        "#CODE RUNS FOR ALMOST 2 HOURS ON GOOGLE COLAB FREE SERVER\n",
        "\n",
        "vae = VAE(\n",
        "    embedding_layer=embedding_layer,\n",
        "    encoder_gru=encoder_gru,\n",
        "    z_mean_dense=z_mean_dense,\n",
        "    z_log_var_dense=z_log_var_dense,\n",
        "    decoder=decoder\n",
        ")\n",
        "vae.compile(optimizer='adam')  # No need to specify loss here\n",
        "vae.fit(train_smiles, epochs=50, batch_size=64, validation_data=(val_smiles,))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Step 3.2: Visualize the Latent Space"
      ],
      "metadata": {
        "id": "YHWtKb-NM_ZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "RqSrjookPrAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rdkit.Chem import Descriptors"
      ],
      "metadata": {
        "id": "JFYB6VogP0p0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tasks:\n",
        "\n",
        "# Use PCA or t-SNE on the latent vectors.\n",
        "\n",
        "# Color-code by molecule properties (e.g., molecular weight or logP if available).\n",
        "\n",
        "# Try clustering.\n",
        "\n",
        "# Goal: Build intuition for how molecules are distributed in latent space."
      ],
      "metadata": {
        "id": "U4NG7V9QL9iF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generating latent vectors for the molecules\n",
        "\n",
        "def generate_latent_vectors(model, data):\n",
        "    \"\"\"Extract latent vectors (z_mean) from the trained VAE model.\"\"\"\n",
        "    # Get the encoder part to generate latent vectors\n",
        "    embedded = model.embedding_layer(data)\n",
        "    _, state_h = model.encoder_gru(embedded)\n",
        "    z_mean = model.z_mean_dense(state_h)\n",
        "    return z_mean.numpy()\n",
        "\n",
        "# Generate latent vectors for all molecules\n",
        "latent_vectors = generate_latent_vectors(vae, padded_smiles)\n",
        "latent_vectors"
      ],
      "metadata": {
        "id": "ldeOgAWVOCM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate molecular properties for color-coding\n",
        "def calculate_mol_properties(smiles_list):\n",
        "    \"\"\"Calculate molecular weight and logP for each SMILES string.\"\"\"\n",
        "    properties = []\n",
        "    for smiles in smiles_list:\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "        if mol:\n",
        "            mw = Descriptors.MolWt(mol)\n",
        "            logp = Descriptors.MolLogP(mol)\n",
        "            properties.append({'MW': mw, 'LogP': logp})\n",
        "        else:\n",
        "            properties.append({'MW': np.nan, 'LogP': np.nan})\n",
        "    return pd.DataFrame(properties)\n",
        "\n",
        "\n",
        "# Extract original SMILES strings\n",
        "original_smiles = zinc_data['SMILES'].tolist()\n",
        "mol_properties = calculate_mol_properties(original_smiles)\n"
      ],
      "metadata": {
        "id": "hOYL05I_QSP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mol_properties"
      ],
      "metadata": {
        "id": "mtrI98jUQ0pe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply PCA\n",
        "pca = PCA(n_components=2)\n",
        "pca_result = pca.fit_transform(latent_vectors)\n",
        "print(f\"PCA explained variance ratio: {pca.explained_variance_ratio_}\")\n"
      ],
      "metadata": {
        "id": "PVVBI6SxRuN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Investigate Latent Space:\n",
        "pca = PCA(n_components=2)\n",
        "pca_result = pca.fit_transform(latent_vectors)\n",
        "\n",
        "# Access the components (loadings)\n",
        "principal_components = pca.components_\n",
        "\n",
        "# The first row of principal_components corresponds to PC1\n",
        "pc1_loadings = principal_components[0]\n",
        "\n",
        "# Now, you can analyze pc1_loadings:\n",
        "# - Print the values to see the contribution of each latent variable to PC1\n",
        "print(pc1_loadings)\n",
        "\n",
        "# - Visualize the loadings using a bar chart or heatmap\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.bar(range(len(pc1_loadings)), pc1_loadings)\n",
        "plt.xlabel(\"Latent Variable Index\")\n",
        "plt.ylabel(\"Loading on PC1\")\n",
        "plt.title(\"Contribution of Latent Variables to PC1\")\n",
        "plt.show()\n",
        "\n",
        "# Larger (absolute) values ​​in pc1_loadingsindicate that the corresponding latent variables contribute more strongly to PC1.\n",
        "# Positive values ​​indicate a positive correlation between the latent variable and PC1.\n",
        "# Negative values ​​indicate a negative correlation."
      ],
      "metadata": {
        "id": "SWmadc8qFfeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply t-SNE\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
        "tsne_result = tsne.fit_transform(latent_vectors)\n",
        "tsne_result\n",
        "\n",
        "#takes 15 mins to run"
      ],
      "metadata": {
        "id": "Yx8HQJfkR-Vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(tsne_result[:, 0], tsne_result[:, 1])\n",
        "plt.xlabel(\"t-SNE Dimension 1\")\n",
        "plt.ylabel(\"t-SNE Dimension 2\")\n",
        "plt.title(\"t-SNE Visualization of Molecules\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "doIZBFxTfPgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# K-Means clustering\n",
        "kmeans = KMeans(n_clusters=5, random_state=42)\n",
        "cluster_labels = kmeans.fit_predict(latent_vectors)\n",
        "cluster_labels\n",
        "#kmeans"
      ],
      "metadata": {
        "id": "bDBN_5EeSIzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization with PCA\n",
        "plt.figure(figsize=(12, 10))\n",
        "\n",
        "# Plot 1: PCA colored by molecular weight\n",
        "plt.subplot(2, 2, 1)\n",
        "scatter = plt.scatter(pca_result[:, 0], pca_result[:, 1], c=mol_properties['MW'],\n",
        "                      cmap='viridis', alpha=0.6, s=10)\n",
        "plt.colorbar(scatter, label='Molecular Weight')\n",
        "plt.title('PCA of Latent Space (colored by MW)')\n",
        "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%})')\n",
        "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%})')\n",
        "\n",
        "# Plot 2: PCA colored by LogP\n",
        "plt.subplot(2, 2, 2)\n",
        "scatter = plt.scatter(pca_result[:, 0], pca_result[:, 1], c=mol_properties['LogP'],\n",
        "                      cmap='plasma', alpha=0.6, s=10)\n",
        "plt.colorbar(scatter, label='LogP')\n",
        "plt.title('PCA of Latent Space (colored by LogP)')\n",
        "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%})')\n",
        "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%})')\n",
        "\n",
        "# Plot 3: PCA with K-means clusters\n",
        "plt.subplot(2, 2, 3)\n",
        "scatter = plt.scatter(pca_result[:, 0], pca_result[:, 1], c=cluster_labels,\n",
        "                      cmap='tab10', alpha=0.6, s=10)\n",
        "plt.colorbar(scatter, label='Cluster')\n",
        "plt.title('PCA of Latent Space with K-means Clusters')\n",
        "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%})')\n",
        "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%})')"
      ],
      "metadata": {
        "id": "3PK0BuMMVEoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Analyze clusters\n",
        "cluster_properties = pd.DataFrame({\n",
        "    'Cluster': cluster_labels,\n",
        "    'MW': mol_properties['MW'],\n",
        "    'LogP': mol_properties['LogP']\n",
        "})\n",
        "\n",
        "# Calculate mean properties for each cluster\n",
        "cluster_stats = cluster_properties.groupby('Cluster').agg(['mean', 'std', 'count'])\n",
        "print(\"Cluster statistics:\")\n",
        "print(cluster_stats)\n",
        "\n",
        "# 9. Additional visualization: Distribution of properties within clusters\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "# Molecular Weight distribution by cluster\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.boxplot(x='Cluster', y='MW', data=cluster_properties)\n",
        "plt.title('Molecular Weight Distribution by Cluster')\n",
        "plt.xlabel('Cluster')\n",
        "plt.ylabel('Molecular Weight')\n",
        "\n",
        "# LogP distribution by cluster\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.boxplot(x='Cluster', y='LogP', data=cluster_properties)\n",
        "plt.title('LogP Distribution by Cluster')\n",
        "plt.xlabel('Cluster')\n",
        "plt.ylabel('LogP')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HeNa_ZU0RaCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. Bonus: Find representative molecules from each cluster\n",
        "def find_representative_molecules(cluster_properties, original_smiles, n=3):\n",
        "    \"\"\"Find n molecules closest to each cluster centroid.\"\"\"\n",
        "    representatives = {}\n",
        "    for cluster_id in np.unique(cluster_properties['Cluster']):\n",
        "        # Get indices of molecules in this cluster\n",
        "        cluster_indices = np.where(cluster_properties['Cluster'] == cluster_id)[0]\n",
        "        # Get subset of latent vectors for this cluster\n",
        "        cluster_vectors = latent_vectors[cluster_indices]\n",
        "        # Get centroid\n",
        "        centroid = np.mean(cluster_vectors, axis=0)\n",
        "        # Calculate distances to centroid\n",
        "        distances = np.linalg.norm(cluster_vectors - centroid, axis=1)\n",
        "        # Get indices of n closest molecules\n",
        "        closest_indices = cluster_indices[np.argsort(distances)[:n]]\n",
        "        # Get SMILES of representative molecules\n",
        "        rep_smiles = [original_smiles[i] for i in closest_indices]\n",
        "        representatives[cluster_id] = rep_smiles\n",
        "    return representatives\n",
        "\n",
        "representative_mols = find_representative_molecules(cluster_properties, original_smiles)\n",
        "print(\"\\nRepresentative molecules from each cluster:\")\n",
        "for cluster_id, smiles_list in representative_mols.items():\n",
        "    print(f\"\\nCluster {cluster_id}:\")\n",
        "    for smiles in smiles_list:\n",
        "        print(f\"  {smiles}\")"
      ],
      "metadata": {
        "id": "VQ3iNymmVMWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uMaz5DJuWA5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Step 3.3: Generate New Molecules"
      ],
      "metadata": {
        "id": "tjCMqqGUWBL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Tasks:\n",
        "\n",
        "Sample new latent vectors.\n",
        "\n",
        "Decode them using your decoder.\n",
        "\n",
        "Convert token sequences back to SMILES.\n",
        "\n",
        "Visualize or validate if they are valid molecules.\n",
        "\n",
        "Goal: Understand generative capability — how plausible are the outputs?"
      ],
      "metadata": {
        "id": "j33N3MXjWBg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tYKH5mc2WQU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Function to sample from latent space\n",
        "def sample_latent_space(n_samples, latent_dim=64):\n",
        "    \"\"\"Sample random points from the latent space\"\"\"\n",
        "    # Standard normal distribution sampling\n",
        "    z_sample = np.random.normal(0, 1, size=(n_samples, latent_dim))\n",
        "    return z_sample\n",
        "\n",
        "# Generate samples from latent space -> latent vectors\n",
        "n_samples = 100\n",
        "latent_samples = sample_latent_space(n_samples, latent_dim=64)\n",
        "latent_samples\n",
        "\n",
        "#latent_dim = 64\n",
        "#\n",
        "# latent_samples = sample_latent_space(n_samples, latent_dim)\n"
      ],
      "metadata": {
        "id": "SDEr0hZDYIAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample from latent space\n",
        "n_samples = 100\n",
        "latent_dim = 64  # Adjust if your latent dimension is different\n",
        "max_length = 100\n",
        "\n",
        "# Improved decoder function for your specific architecture\n",
        "def decode_latent_vectors(vae_model, z_vectors, max_length):\n",
        "    \"\"\"Decode latent vectors to token sequences\"\"\"\n",
        "    # Direct decoding using the decoder component\n",
        "    # This assumes your decoder can take the latent vectors directly\n",
        "    output_tokens = vae_model.decoder(z_vectors)\n",
        "\n",
        "    # Get the most likely token for each position\n",
        "    # output_tokens shape should be (batch_size, seq_length, vocab_size)\n",
        "    token_indices = np.argmax(output_tokens, axis=-1)\n",
        "\n",
        "    # Truncate to max_length if needed\n",
        "    if token_indices.shape[1] > max_length:\n",
        "        token_indices = token_indices[:, :max_length]\n",
        "\n",
        "    return token_indices\n",
        "\n",
        "# Run the decoding\n",
        "decoded_sequences = decode_latent_vectors(vae, latent_samples, max_length)\n",
        "decoded_sequences"
      ],
      "metadata": {
        "id": "xN5ppA6Cvp4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # 2. Function to decode latent vectors back to token sequences\n",
        "# def decode_latent_vectors(decoder_model, z_vectors):\n",
        "#     \"\"\"Decode latent vectors to token sequences\"\"\"\n",
        "#     # Use the decoder to generate token probabilities\n",
        "#     token_probs = decoder_model.predict(z_vectors)\n",
        "\n",
        "#     # Get the most likely token for each position\n",
        "#     token_indices = np.argmax(token_probs, axis=-1)\n",
        "#     return token_indices\n",
        "\n",
        "# # Decode the latent vectors using your decoder\n",
        "# # Since we have the full VAE model, we need to use just the decoder part\n",
        "# decoded_sequences = decode_latent_vectors(vae.decoder, latent_samples)\n",
        "# decoded_sequences\n"
      ],
      "metadata": {
        "id": "sC0sfgm1bBFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Function to convert token indices back to SMILES strings\n",
        "# def indices_to_smiles(token_indices, idx_to_token):\n",
        "#     \"\"\"Convert token indices back to SMILES strings\"\"\"\n",
        "#     smiles_list = []\n",
        "\n",
        "#     for seq in token_indices:\n",
        "#         # Convert indices to tokens\n",
        "#         tokens = [idx_to_token.get(idx, '') for idx in seq if idx != 0]  # Skip padding tokens\n",
        "\n",
        "#         # Join tokens to form SMILES string\n",
        "#         smiles = ''.join(tokens)\n",
        "\n",
        "#         # Find where the SMILES string might end (if there's a termination token)\n",
        "#         # Adjust this depending on your tokenization scheme\n",
        "#         if '\\n' in smiles:\n",
        "#             smiles = smiles.split('\\n')[0]\n",
        "\n",
        "#         smiles_list.append(smiles)\n",
        "\n",
        "#     return smiles_list\n",
        "\n",
        "\n",
        "# 3. Improved function to convert token indices to SMILES\n",
        "def indices_to_smiles(token_indices, idx_to_token, pad_token=0):\n",
        "    \"\"\"Convert token indices back to SMILES strings with better handling\"\"\"\n",
        "    smiles_list = []\n",
        "\n",
        "    for seq in token_indices:\n",
        "        # Filter out padding and end tokens\n",
        "        valid_indices = [idx for idx in seq if idx != pad_token]\n",
        "\n",
        "        # Convert indices to tokens\n",
        "        tokens = [idx_to_token.get(idx, '') for idx in valid_indices]\n",
        "\n",
        "        # Join tokens - handling depends on your tokenization approach\n",
        "        if isinstance(tokens[0], str) and len(tokens[0]) == 1:\n",
        "            # Character-level tokenization\n",
        "            smiles = ''.join(tokens)\n",
        "        else:\n",
        "            # Token-level tokenization (with spaces between tokens)\n",
        "            smiles = ''.join(tokens)\n",
        "\n",
        "        # Check for termination token and truncate\n",
        "        if '\\n' in smiles:\n",
        "            smiles = smiles.split('\\n')[0]\n",
        "\n",
        "        smiles_list.append(smiles)\n",
        "\n",
        "    return smiles_list\n",
        "\n",
        "\n",
        "# def indices_to_smiles(token_indices, idx_to_token):\n",
        "#     \"\"\"Convert token indices back to SMILES strings\"\"\"\n",
        "#     smiles_list = []\n",
        "\n",
        "#     for seq in token_indices:\n",
        "#         smiles = \"\"\n",
        "#         for idx in seq:\n",
        "#             if idx != 0:  # Skip padding tokens\n",
        "#                 token = idx_to_token.get(idx, '')\n",
        "#                 # Add space if it's in the vocabulary and the previous token was not a space\n",
        "#                 if token == ' ' and smiles and smiles[-1] != ' ':\n",
        "#                     smiles += token\n",
        "#                 elif token != ' ':\n",
        "#                     smiles += token\n",
        "\n",
        "#         # Find where the SMILES string might end (if there's a termination token)\n",
        "#         if '\\n' in smiles:\n",
        "#             smiles = smiles.split('\\n')[0]\n",
        "\n",
        "#         smiles_list.append(smiles)\n",
        "\n",
        "#     return smiles_list\n",
        "\n",
        "# First, create the inverse vocabulary (index to token mapping)\n",
        "idx_to_token = {idx: token for token, idx in vocab.items()}\n",
        "\n",
        "# Convert token indices back to SMILES strings\n",
        "generated_smiles = indices_to_smiles(decoded_sequences, idx_to_token)\n",
        "generated_smiles[:9]\n"
      ],
      "metadata": {
        "id": "d71owQJzbCnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # To generate new SMILES\n",
        "\n",
        "# def generate_smiles(model, latent_dim, char_to_index, index_to_char, max_length):\n",
        "#     # Sample a random point in the latent space\n",
        "#     z = tf.random.normal(shape=(1, latent_dim))\n",
        "\n",
        "#     # Initial input token (typically start token or padding)\n",
        "#     current_token = np.zeros((1, 1))  # Use your start token index here\n",
        "\n",
        "#     generated_tokens = []\n",
        "\n",
        "#     # Generate tokens sequentially\n",
        "#     for i in range(max_length):\n",
        "#         # Predict the next token probabilities\n",
        "#         predictions = model.decoder([z, current_token])\n",
        "\n",
        "#         # Sample from the predictions\n",
        "#         sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "\n",
        "#         # Break if end token is generated\n",
        "#         if sampled_token_index == char_to_index.get('\\n', 0):  # Or whatever your end token is\n",
        "#             break\n",
        "\n",
        "#         # Add the token to our generated sequence\n",
        "#         generated_tokens.append(index_to_char[sampled_token_index])\n",
        "\n",
        "#         # Update current token for next prediction if needed\n",
        "#         if model.decoder.stateful:\n",
        "#             current_token = np.array([[sampled_token_index]])\n",
        "\n",
        "#     # Join tokens into a SMILES string\n",
        "#     generated_smiles = ''.join(generated_tokens)\n",
        "#     return generated_smiles\n",
        "\n",
        "# # Convert token indices back to SMILES strings\n",
        "# generated_smiles = indices_to_smiles(decoded_sequences, idx_to_token)\n",
        "# generated_smiles[:9]\n"
      ],
      "metadata": {
        "id": "lqB6UKtWmzXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# # 3. Improved function to convert token indices to SMILES\n",
        "# def indices_to_smiles(token_indices, idx_to_token, pad_token=0):\n",
        "#     \"\"\"Convert token indices back to SMILES strings with better handling\"\"\"\n",
        "#     smiles_list = []\n",
        "\n",
        "#     for seq in token_indices:\n",
        "#         # Filter out padding and end tokens\n",
        "#         valid_indices = [idx for idx in seq if idx != pad_token]\n",
        "\n",
        "#         # Convert indices to tokens\n",
        "#         tokens = [idx_to_token.get(idx, '') for idx in valid_indices]\n",
        "\n",
        "#         # Join tokens - handling depends on your tokenization approach\n",
        "#         if isinstance(tokens[0], str) and len(tokens[0]) == 1:\n",
        "#             # Character-level tokenization\n",
        "#             smiles = ''.join(tokens)\n",
        "#         else:\n",
        "#             # Token-level tokenization (with spaces between tokens)\n",
        "#             smiles = ''.join(tokens)\n",
        "\n",
        "#         # Check for termination token and truncate\n",
        "#         if '\\n' in smiles:\n",
        "#             smiles = smiles.split('\\n')[0]\n",
        "\n",
        "#         smiles_list.append(smiles)\n",
        "\n",
        "#     return smiles_list"
      ],
      "metadata": {
        "id": "6MjILiSeqnX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qc8Utlhoy8Wy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uG6VjA6ddM-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Function to check validity of generated SMILES\n",
        "def check_validity(smiles_list):\n",
        "    \"\"\"Check if generated SMILES strings are valid molcules\"\"\"\n",
        "    valid_mols = []\n",
        "    valid_smiles = []\n",
        "\n",
        "    for smiles in smiles_list:\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "        if mol is not None:\n",
        "            valid_mols.append(mol)\n",
        "            valid_smiles.append(smiles)\n",
        "\n",
        "    print(f\"Generated {len(smiles_list)} SMILES strings\")\n",
        "    print(f\"Valid molecules: {len(valid_mols)} ({len(valid_mols)/len(smiles_list)*100:.2f}%)\")\n",
        "\n",
        "    return valid_mols, valid_smiles\n",
        "\n",
        "# Check validity and get valid molecules\n",
        "valid_mols, valid_smiles = check_validity(generated_smiles)"
      ],
      "metadata": {
        "id": "FZ-FnwBEbEdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Visualize some of the valid molecules\n",
        "def visualize_molecules(mols, n_per_row=5, n_rows=4):\n",
        "    \"\"\"Display a grid of molecules\"\"\"\n",
        "    # Take up to n_per_row * n_rows molecules\n",
        "    mols_to_display = mols[:n_per_row * n_rows]\n",
        "\n",
        "    if len(mols_to_display) == 0:\n",
        "        print(\"No valid molecules to display!\")\n",
        "        return\n",
        "\n",
        "    # Create a molecule grid\n",
        "    img = Draw.MolsToGridImage(\n",
        "        mols_to_display,\n",
        "        molsPerRow=n_per_row,\n",
        "        subImgSize=(200, 200),\n",
        "        legends=[f\"Mol {i+1}\" for i in range(len(mols_to_display))]\n",
        "    )\n",
        "\n",
        "    # Display the image\n",
        "    plt.figure(figsize=(15, 12))\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Generated Valid Molecules\")\n",
        "    plt.show()\n",
        "\n",
        "# Visualize the valid molecules\n",
        "visualize_molecules(valid_mols)\n"
      ],
      "metadata": {
        "id": "B4CfFU7Faqsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 6. Calculate basic properties of valid molecules\n",
        "def calculate_properties(mols):\n",
        "    \"\"\"Calculate some basic molecular properties\"\"\"\n",
        "    from rdkit.Chem import Descriptors\n",
        "\n",
        "    properties = {\n",
        "        'MW': [],       # Molecular Weight\n",
        "        'LogP': [],     # Partition coefficient\n",
        "        'HBA': [],      # Hydrogen Bond Acceptors\n",
        "        'HBD': [],      # Hydrogen Bond Donors\n",
        "        'RotBonds': [], # Rotatable Bonds\n",
        "        'Rings': []     # Ring Count\n",
        "    }\n",
        "\n",
        "    for mol in mols:\n",
        "        properties['MW'].append(Descriptors.MolWt(mol))\n",
        "        properties['LogP'].append(Descriptors.MolLogP(mol))\n",
        "        properties['HBA'].append(Descriptors.NumHAcceptors(mol))\n",
        "        properties['HBD'].append(Descriptors.NumHDonors(mol))\n",
        "        properties['RotBonds'].append(Descriptors.NumRotatableBonds(mol))\n",
        "        properties['Rings'].append(Descriptors.RingCount(mol))\n",
        "\n",
        "    return properties\n",
        "\n",
        "# Calculate properties of the generated molecules\n",
        "generated_properties = calculate_properties(valid_mols)\n"
      ],
      "metadata": {
        "id": "UJmeF51paZLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ha2GZR4EZ_3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 7. Compare to training data properties\n",
        "# Assuming you have access to the original zinc_data\n",
        "original_mols = [Chem.MolFromSmiles(smiles) for smiles in original_smiles]\n",
        "original_valid_mols = [mol for mol in original_mols if mol is not None]\n",
        "original_properties = calculate_properties(original_valid_mols)\n",
        "\n",
        "# 8. Plot distribution comparisons\n",
        "def plot_property_comparison(original_props, generated_props):\n",
        "    \"\"\"Plot histograms comparing original and generated molecule properties\"\"\"\n",
        "    properties = ['MW', 'LogP', 'HBA', 'HBD', 'RotBonds', 'Rings']\n",
        "    fig, axes = plt.subplots(3, 2, figsize=(15, 12))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i, prop in enumerate(properties):\n",
        "        ax = axes[i]\n",
        "        ax.hist(original_props[prop], alpha=0.5, bins=20, label='Training Data')\n",
        "        ax.hist(generated_props[prop], alpha=0.5, bins=20, label='Generated')\n",
        "        ax.set_title(f'{prop} Distribution')\n",
        "        ax.set_xlabel(prop)\n",
        "        ax.set_ylabel('Count')\n",
        "        ax.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot the property comparisons\n",
        "plot_property_comparison(original_properties, generated_properties)\n",
        "\n",
        "# 9. Advanced Analysis: Chemical Similarity\n",
        "def analyze_similarity(original_mols, generated_mols, sample_size=100):\n",
        "    \"\"\"Analyze chemical similarity between generated and original molecules\"\"\"\n",
        "    from rdkit import DataStructs\n",
        "    from rdkit.Chem import AllChem\n",
        "    import random\n",
        "\n",
        "    # If we have too many molecules, sample them\n",
        "    if len(original_mols) > sample_size:\n",
        "        original_sample = random.sample(original_mols, sample_size)\n",
        "    else:\n",
        "        original_sample = original_mols\n",
        "\n",
        "    if len(generated_mols) > sample_size:\n",
        "        generated_sample = random.sample(generated_mols, sample_size)\n",
        "    else:\n",
        "        generated_sample = generated_mols\n",
        "\n",
        "    # Calculate Morgan fingerprints for all molecules\n",
        "    original_fps = [AllChem.GetMorganFingerprintAsBitVect(mol, 2) for mol in original_sample]\n",
        "    generated_fps = [AllChem.GetMorganFingerprintAsBitVect(mol, 2) for mol in generated_sample]\n",
        "\n",
        "    # Calculate all pairwise similarities between generated and original\n",
        "    similarities = []\n",
        "    for gen_fp in generated_fps:\n",
        "        for orig_fp in original_fps:\n",
        "            sim = DataStructs.TanimotoSimilarity(gen_fp, orig_fp)\n",
        "            similarities.append(sim)\n",
        "\n",
        "    # Calculate internal similarities within generated set\n",
        "    internal_similarities = []\n",
        "    for i in range(len(generated_fps)):\n",
        "        for j in range(i+1, len(generated_fps)):\n",
        "            sim = DataStructs.TanimotoSimilarity(generated_fps[i], generated_fps[j])\n",
        "            internal_similarities.append(sim)\n",
        "\n",
        "    # Plot similarity distributions\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(similarities, alpha=0.5, bins=20, label='Gen-to-Original')\n",
        "    plt.hist(internal_similarities, alpha=0.5, bins=20, label='Gen-to-Gen')\n",
        "    plt.title('Chemical Similarity Distributions (Tanimoto)')\n",
        "    plt.xlabel('Tanimoto Similarity')\n",
        "    plt.ylabel('Count')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return np.mean(similarities), np.mean(internal_similarities)\n",
        "\n",
        "# Calculate and plot similarity metrics\n",
        "avg_cross_sim, avg_internal_sim = analyze_similarity(original_valid_mols, valid_mols)\n",
        "print(f\"Average similarity to training data: {avg_cross_sim:.3f}\")\n",
        "print(f\"Average internal similarity: {avg_internal_sim:.3f}\")\n",
        "\n",
        "# # 10. Latent Space Interpolation between molecules\n",
        "# def interpolate_molecules(vae_model, start_smiles, end_smiles, n_steps=10):\n",
        "#     \"\"\"Generate molecules by interpolating in latent space between two SMILES strings\"\"\"\n",
        "#     # Tokenize both SMILES\n",
        "#     start_tokens = tokenize_smiles(start_smiles)\n",
        "#     end_tokens = tokenize_smiles(end_smiles)\n",
        "\n",
        "#     # Encode to token indices\n",
        "#     start_indices = [vocab[token] for token in start_tokens]\n",
        "#     end_indices = [vocab[token] for token in end_tokens]\n",
        "\n",
        "#     # Pad sequences\n",
        "#     start_padded = keras.preprocessing.sequence.pad_sequences([start_indices], maxlen=max_len, padding='post', value=0)\n",
        "#     end_padded = keras.preprocessing.sequence.pad_sequences([end_indices], maxlen=max_len, padding='post', value=0)\n",
        "\n",
        "#     # Get latent vectors\n",
        "#     start_latent = generate_latent_vectors(vae, start_padded)\n",
        "#     end_latent = generate_latent_vectors(vae, end_padded)\n",
        "\n",
        "#     # Create interpolations\n",
        "#     alphas = np.linspace(0, 1, n_steps)\n",
        "#     interp_latent = np.array([(1-alpha)*start_latent[0] + alpha*end_latent[0] for alpha in alphas])\n",
        "\n",
        "#     # Decode interpolated vectors\n",
        "#     interp_sequences = decode_latent_vectors(vae_model, interp_latent, max_length)\n",
        "#     interp_smiles = indices_to_smiles(interp_sequences, idx_to_token)\n",
        "\n",
        "#     # Convert to molecules\n",
        "#     interp_mols = []\n",
        "#     valid_interp_smiles = []\n",
        "#     for smiles in interp_smiles:\n",
        "#         mol = Chem.MolFromSmiles(smiles)\n",
        "#         if mol is not None:\n",
        "#             interp_mols.append(mol)\n",
        "#             valid_interp_smiles.append(smiles)\n",
        "\n",
        "#     return interp_mols, valid_interp_smiles\n",
        "\n",
        "# # Select two valid SMILES from the training data to interpolate between\n",
        "# if len(original_smiles) >= 2:\n",
        "#     start_smiles = original_smiles[0]\n",
        "#     end_smiles = original_smiles[-1]\n",
        "\n",
        "#     interp_mols, interp_smiles = interpolate_molecules(vae, start_smiles, end_smiles, n_steps=8)\n",
        "\n",
        "#     # Visualize the interpolation pathway\n",
        "#     if len(interp_mols) > 0:\n",
        "#         img = Draw.MolsToGridImage(\n",
        "#             interp_mols,\n",
        "#             molsPerRow=4,\n",
        "#             subImgSize=(200, 200),\n",
        "#             legends=[f\"Step {i}\" for i in range(len(interp_mols))]\n",
        "#         )\n",
        "\n",
        "#         plt.figure(figsize=(15, 8))\n",
        "#         plt.imshow(img)\n",
        "#         plt.axis('off')\n",
        "#         plt.title(\"Molecule Interpolation in Latent Space\")\n",
        "#         plt.show()\n",
        "\n",
        "#         # Print the SMILES\n",
        "#         print(\"Interpolation SMILES:\")\n",
        "#         for i, smiles in enumerate(interp_smiles):\n",
        "#             print(f\"Step {i}: {smiles}\")\n",
        "#     else:\n",
        "#         print(\"No valid molecules found in the interpolation path.\")"
      ],
      "metadata": {
        "id": "w34qksJIZ_m1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sTmeTNcZ72gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ** HELPFUL LINKS **"
      ],
      "metadata": {
        "id": "QLl1dNip-Gim"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "1. https://github.com/pyg-team/pytorch_geometric/discussions/3451\n",
        "\n",
        "2. https://github.com/wengong-jin/icml18-jtnn/tree/master/data/zinc\n",
        "\n",
        "3. https://github.com/wengong-jin/icml18-jtnn/blob/28ed03fcb3f0a79f44f73eefc0bcad613ea39167/jtnn/mpn.py#L26-L31\n",
        "\n",
        "4. https://github.com/wengong-jin/icml18-jtnn/blob/master/README.md\n",
        "\n",
        "5. https://docs.nvidia.com/bionemo-framework/1.10/notebooks/ZINC15-data-preprocessing.html"
      ],
      "metadata": {
        "id": "d1mj8ZH19-TQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('a')"
      ],
      "metadata": {
        "id": "lUNnOXxm95PZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LtPHP2dn-bFc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "rI7-J9PeOisv",
        "kHRj1nqGvBNP",
        "61223hzPDQNi",
        "hLP0eL38Hvz_",
        "o9T0nN5di9EC"
      ],
      "provenance": [],
      "authorship_tag": "ABX9TyPcI1fLNq2n4CyTBfbqVptK",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}