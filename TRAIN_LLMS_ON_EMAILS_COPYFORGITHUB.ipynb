{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOLeL2Iz7M3ClLJXWN4XIje",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SamuelRuby/DeepLearningProjects/blob/main/TRAIN_LLMS_ON_EMAILS_COPYFORGITHUB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "0aDCq6Lrm5QE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sumy"
      ],
      "metadata": {
        "id": "7vKzhmJomFQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -qU\n",
        "\n"
      ],
      "metadata": {
        "id": "WqGa1gc-KNVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#------------------#\n",
        "# LIBRARIES\n",
        "#-----------------------#\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "import re\n",
        "import zipfile\n",
        "import mailbox\n",
        "from bs4 import BeautifulSoup  # Install if needed: !pip install beautifulsoup4\n",
        "import csv\n",
        "import json\n",
        "import unicodedata\n",
        "#import nltk #.tokenize import word_tokenize\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import wandb\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "5Za5tJ5avVjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "igqZ_NrZtDNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AtEKaqbws7lS"
      },
      "outputs": [],
      "source": [
        "os.chdir('/content/drive/MyDrive/Takeout')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#unzipped\n",
        "zip_file_path = 'takeout-20250401T135415Z-001.zip'\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall()\n",
        "print(\"Zip file extracted successfully!\")"
      ],
      "metadata": {
        "id": "CvAAu6TfwH8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_list = os.listdir()\n",
        "print(file_list)"
      ],
      "metadata": {
        "id": "2JEgiwWQvpuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd Takeout\n",
        "\n",
        "!ls"
      ],
      "metadata": {
        "id": "MJ8vMDIjwtSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-zK-3VPL5OYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir('Takeout/Mail')"
      ],
      "metadata": {
        "id": "933BAxkywvQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_body(msg):\n",
        "    if msg.is_multipart():\n",
        "        for part in msg.walk():\n",
        "            content_type = part.get_content_type()\n",
        "            if content_type == \"text/plain\":\n",
        "                return part.get_payload(decode=True).decode('utf-8', errors='ignore')\n",
        "            elif content_type == \"text/html\":\n",
        "                soup = BeautifulSoup(part.get_payload(decode=True), \"html.parser\")\n",
        "                return soup.get_text()\n",
        "    else:\n",
        "        return msg.get_payload(decode=True).decode('utf-8', errors='ignore')\n"
      ],
      "metadata": {
        "id": "-9dG0Opc_Rjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the path to the .mbox file:\n",
        "mbox_file_path = '/content/drive/MyDrive/Takeout/Takeout/Mail/All mail Including Spam and Trash.mbox'\n",
        "\n",
        "# mbox object:\n",
        "mbox = mailbox.mbox(mbox_file_path)\n",
        "\n",
        "# Iterate through messages:\n",
        "for message in mbox:\n",
        "  #print(message)\n",
        "    # Access message headers and content, and labels\n",
        "    subject = message['subject']\n",
        "    sender = message['from']\n",
        "    x_gmail_labels = message['X-Gmail-Labels']\n",
        "    body = extract_body(message)#.get_payload()\n",
        "    #delivered_to = message['Delivered-To']\n",
        "    #received = message['Received']\n",
        "\n",
        "    # print(f\"Subject: {subject}\")\n",
        "    # print(f\"Sender: {sender}\")\n",
        "    # print(f\"X-Gmail-Labels: {x_gmail_labels}\")\n",
        "    # print(f\"Body: {body[:200]}\")\n",
        "    # #print(f\"Delivered-To: {delivered_to}\")\n",
        "    # #print(f\"Received: {received}\")\n",
        "    # print(\"-\" * 50)  # Separator\n",
        "\n",
        "\n",
        "# X-Gmail-Labels: Archived,Important,Opened,Category personal,We could have\n",
        "# Delivered-To: workwithrubyy@gmail.com\n",
        "# Received: by\n",
        "# Subject: Subject: Content-Type: Mime-Version: Date: Sender: Sender:\n",
        "#  To: From: From: Subject: Subject: Content-Type: Mime-Version: Date:\n",
        "#  Sender: Sender: X-Feedback-Id;\n",
        "# X-Mailgun-Tag: CandidateAutoReply\n",
        "# List-Unsubscribe:\n",
        "# Message-Id:"
      ],
      "metadata": {
        "id": "8Fnzj_Laxwmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save as csv\n",
        "#output_file = \"/content/drive/MyDrive/Takeout/emails.csv\"\n",
        "\n",
        "# with open(output_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "#     writer = csv.writer(f)\n",
        "#     writer.writerow([\"Subject\", \"Sender\", \"Labels\", \"Body\"])  # Column headers\n",
        "\n",
        "#     for message in mbox:\n",
        "#         subject = message[\"subject\"]\n",
        "#         sender = message[\"from\"]\n",
        "#         x_gmail_labels = message[\"X-Gmail-Labels\"]\n",
        "#         body = extract_body(message)\n",
        "\n",
        "#         writer.writerow([subject, sender, x_gmail_labels, body])\n",
        "\n",
        "# print(f\"Emails saved to {output_file}\")\n"
      ],
      "metadata": {
        "id": "GuX68mDi0uZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save as json\n",
        "emails = []\n",
        "\n",
        "for message in mbox:\n",
        "    email_data = {\n",
        "        \"subject\": message[\"subject\"],\n",
        "        \"sender\": message[\"from\"],\n",
        "        \"labels\": message[\"X-Gmail-Labels\"],\n",
        "        \"body\": extract_body(message),\n",
        "    }\n",
        "    emails.append(email_data)\n",
        "\n",
        "output_file = \"/content/drive/MyDrive/Takeout/emails.json\"\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(emails, f, indent=4)\n",
        "\n",
        "print(f\"Emails saved to {output_file}\")\n"
      ],
      "metadata": {
        "id": "K6hVNh9KA2mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Filter emails with null or empty subject\n",
        "# filtered_emails = [email for email in emails\n",
        "#                    if email.get(\"subject\") is None or email.get(\"subject\") == \"\"]\n",
        "\n",
        "# # Print the filtered emails (or do something else with them)\n",
        "# for email in filtered_emails:\n",
        "#     print(email)"
      ],
      "metadata": {
        "id": "4THaSIhkL6ee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sCUf-Dh6L6Um"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jcEUZpuyrIJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2ND STEP: PREPROCESSING"
      ],
      "metadata": {
        "id": "dMQ3ChCiD3C0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_email_text(text):\n",
        "  soup = BeautifulSoup(text, \"html.parser\") # Removes HTML tags\n",
        "  text = soup.get_text()\n",
        "\n",
        "  # Remove signatures and disclaimers using regex\n",
        "  text = re.sub(r'(?i)(\\n?--\\n?.+|Thanks,?.*|Sincerely,?.*|Regards,?.*|Best,?.*|Unsubscribe.*|preferences.*)', '', text, flags=re.DOTALL)\n",
        "  #text = re.sub(r'(?i)(Thanks,|Sincerely,|Regards,|Best,|Unsubscribe,|preferences,).*', '', text) # Common signature patterns\n",
        "  text = re.sub(r'(?i)This email.*confidential.*', '', text, flags=re.DOTALL)  # Disclaimer example\n",
        "\n",
        "  # Remove URLs using regex\n",
        "  text = re.sub(r'http\\S+', '', text)\n",
        "\n",
        "  # Remove extra whitespace and newlines\n",
        "  text = ' '.join(text.split())\n",
        "\n",
        "  #   # Remove HTML tags:\n",
        "  # if \"<\" in text and \">\" in text:  # Check if HTML is likely present\n",
        "  #     soup = BeautifulSoup(text, \"html.parser\")\n",
        "  #     text = soup.get_text()\n",
        "\n",
        "  # Remove non-ASCII characters:\n",
        "  text = text.encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
        "\n",
        "  # Normalize Unicode characters (e.g., fancy quotes, right-single quote, non-breaking spaces)\n",
        "  text = unicodedata.normalize(\"NFKD\", text)\n",
        "\n",
        "  # Remove or replace emojis (using regex):\n",
        "  text = re.sub(r'[\\U0001F600-\\U0001F64F]', '', text)  # removes all emojis\n",
        "  # Or to remove them entirely:\n",
        "  # text = re.sub(r'[\\U0001F600-\\U0001F64F]', '', text)\n",
        "\n",
        "    # ... (Add more cleaning steps as needed) ...\n",
        "  return text\n",
        "\n",
        "\n",
        "# for email in emails:\n",
        "#     email['body'] = clean_email_text(email['body'])"
      ],
      "metadata": {
        "id": "521JEqZYCpHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TOKENIZE AND VECTORIZATION\n",
        "#NOTE: Ceell takes up to 15 mins to run, so don't run often\n",
        "\n",
        "# nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "# for email in emails:\n",
        "#     email[\"cleaned_body\"] = clean_email_text(email[\"body\"])  # Clean the body text\n",
        "#     email[\"tokens\"] = word_tokenize(email[\"cleaned_body\"])  # Tokenize\n",
        "\n",
        "# # Save the updated data with cleaned text and tokens\n",
        "# output_file = \"/content/drive/MyDrive/Takeout/emails_cleaned_tokenized.json\"\n",
        "# with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "#     json.dump(emails, f, indent=4)\n",
        "\n",
        "# print(f\"Cleaned emails saved to {output_file}\")\n",
        "\n",
        "\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "\n",
        "\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\") #have to chnage to distilbert\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# Load pre-trained tokenizer and model:\n",
        "model = AutoModel.from_pretrained(model_name)  # Use AutoModel for embedding generation\n",
        "\n",
        "\n",
        "# CLEAN TEXT (using function) AND TOKENIZE\n",
        "for email in emails:\n",
        "    email[\"cleaned_body\"] = clean_email_text(email[\"body\"])  # Clean text\n",
        "\n",
        "    # Tokenize using BERT's tokenizer\n",
        "    email[\"encoded\"] = tokenizer(\n",
        "        email[\"cleaned_body\"],\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "#VECTORIZE\n",
        "def tvectorize(tokenized_input):\n",
        "  with torch.no_grad(): #saves memory cuz no gradient computation\n",
        "    outputs = model(**tokenized_input)\n",
        "  embeddings = outputs.last_hidden_state[:, 0, :].detach().numpy()  # Get [CLS] token embedding #converts the embedding from a PyTorch tensor to a NumPy array.\n",
        "  return embeddings\n",
        "\n",
        "# Apply:\n",
        "# using BERT to convert the text of each email into a meaningful numerical vector representation (embedding).\n",
        "for email in emails:\n",
        "    email['embeddings'] = tvectorize(email['encoded'])"
      ],
      "metadata": {
        "id": "VNzhC70KIMvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "email[\"cleaned_body\"]"
      ],
      "metadata": {
        "id": "5A-ew0l2L-jG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#email[\"encoded\"]"
      ],
      "metadata": {
        "id": "V9v6YmTrvxtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# email['embeddings']"
      ],
      "metadata": {
        "id": "xujkZwnwNzvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# missing_subjects = [email for email in emails if email.get(\"subject\") is None]\n",
        "# print(f\"Number of emails with missing subjects: {len(missing_subjects)}\")\n",
        "\n",
        "# for email in missing_subjects[:5]:  # Show first 5\n",
        "#     print(email)\n",
        "\n"
      ],
      "metadata": {
        "id": "fYeAZATZrXnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iaS9TfpQo8r-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OLXdvQcipEaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#defining labels, # make sure to ignore system labels (inbox, starred...)\n",
        "\n",
        "#mapping each email's labels to numbers\n",
        "def assign_label(labels, label_mapping, email):\n",
        "    \"\"\"Assigns a label to an email based on existing labels or content-based logic.\"\"\"\n",
        "\n",
        "    # 1Ô∏è‚É£ Try using predefined labels first\n",
        "    for label in labels.split(\",\"):\n",
        "        label = label.strip()  # Remove spaces or newline characters\n",
        "        if label in label_mapping:\n",
        "            return label_mapping[label]  # Return the first matching label\n",
        "\n",
        "    # 2Ô∏è‚É£ Use subject/body-based logic if no label is found\n",
        "    subject = (email.get(\"subject\") or \"\").lower()\n",
        "    body = (email.get(\"cleaned_body\") or \"\").lower() # Use .get() with a default value\n",
        "\n",
        "\n",
        "    if any(word in subject for word in [\"regret\", \"we regret\", \"declined\", \"not moving forward\"]) or \\\n",
        "       any(word in body for word in [\"regret\", \"pursue other candidates\", \"to move forward\", \"not selected\", \"unsuccessful\", \"unfortunately\"]):\n",
        "        return label_mapping[\"We could have\"]\n",
        "\n",
        "    elif any(word in subject for word in [\"interview\", \"assessment\", \"invitation\", \"schedule\"]) or \\\n",
        "         any(word in body for word in [\"interview\", \"assessment\", \"invitation\", \"schedule\", \"time slot\", \"HR meeting\", \"meeting scheduled\"]):\n",
        "        return label_mapping[\"Interviews\"]\n",
        "\n",
        "    elif \"offer\" in subject or \"offer\" in body:\n",
        "        return label_mapping[\"Offer\"]\n",
        "\n",
        "    elif any(word in subject for word in [\"order confirmation\", \"shipment\", \"your package\", \"env√≠o\", \"order\", \"order status\"]) or \\\n",
        "         any(word in body for word in [\"receipt\", \"pedido\", \"tracking number\", \"order number\", \"tracking\", \"delivery\", \"purchase\", \"refund\", \"order\", \"order status\", \"delivery expected\", \"discount\", \"promo code\", \"order status\", \"return policy\"]):\n",
        "        return label_mapping[\"Shopping\"]\n",
        "\n",
        "    elif any(word in subject for word in [\"invoice\", \"payment due\", \"billing statement\", \"tax\", \"refund\", \"salary\", \"bank statement\"]) or \\\n",
        "         any(word in body for word in [\"payment reminder\", \"amount due\", \"overdue invoice\", \"tax\", \"refund\", \"salary\", \"bank statement\"]):\n",
        "        return label_mapping[\"Finances\"]\n",
        "\n",
        "    elif any(word in subject for word in [\"exam\", \"class schedule\", \"course enrollment\"]) or \\\n",
        "         any(word in body for word in [\"assignment\", \"university\", \"professor\", \"grades\", \"lecture\", \"syllabus\", \"research\", \"homework\", \"enrollment\", \"course\", \"semester\", \"assignment deadline\"]):\n",
        "        return label_mapping[\"School\"]\n",
        "\n",
        "    elif any(word in subject for word in [\"application\", \"thank you\", \"your submission\", \"cover letter\"]) or \\\n",
        "         any(word in body for word in [\"thank you for your application\", \"application\", \"we received your application\", \"cover letter\", \"resume submitted\", \"position applied\"]):\n",
        "        return label_mapping[\"Applications\"]\n",
        "\n",
        "    elif any(word in subject for word in [\"newsletter\", \"subscribe\", \"weekly update\", \"product update\", \"new feature\", \"community news\"]) or \\\n",
        "         any(word in body for word in [\"unsubscribe\", \"manage preferences\", \"stay informed\", \"product update\", \"new feature\", \"community news\"]):\n",
        "        return label_mapping[\"Updates\"]\n",
        "\n",
        "    return label_mapping.get(\"Other\", -1)  # If no match is found, default to 'Other'\n",
        "\n",
        "#define label mappings\n",
        "label_mapping = {\n",
        "    \"Shopping\": 0,\n",
        "    \"Updates\": 1,\n",
        "    \"School\": 2,\n",
        "    \"We could have\": 3,\n",
        "    \"Socials\": 4,\n",
        "    \"Forums\": 5,\n",
        "    \"Promotions\": 6,\n",
        "    \"Interviews\": 7,\n",
        "    \"Offer\": 8,\n",
        "    \"Finances\": 9,\n",
        "    \"Applications\": 10,\n",
        "    \"Other\": 11\n",
        "}\n",
        "\n",
        "# Apply to each email:\n",
        "for email in emails:\n",
        "    email[\"numeric_label\"] = assign_label(email[\"labels\"], label_mapping, email)\n"
      ],
      "metadata": {
        "id": "1fHuKrKMuYpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qfvk1Edque6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FINE TUNE A SMALL TRANSFORMER MODEL**\n",
        " - Choose model\n",
        " - train it on required tasks\n",
        " - evaluation and finetuning (tweaking)\n",
        "\n",
        "MODEL REQUIREMENTS:\n",
        "- One that's fast and efficient,\n",
        "- capable of sequence classification,\n",
        "- can be fine-tuned for email replies and few-shot learning, and\n",
        "- is suitable for summarizing long email threads."
      ],
      "metadata": {
        "id": "ysVdNo1swjR1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U_iCqZHs2zVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**choices**:\n",
        "\n",
        "- DistilBERT: retains much of BERT's performance on various tasks, including sequence classification\n",
        "BUT may not be the absoulte best for very long sequences or complicated tasks\n",
        "\n",
        "- longformer: can handle much longer sequences than traditional transformers, making it potentially well-suited for long email threads and summarization tasks\n",
        "BUT needs more computational resources\n",
        "\n",
        "\n",
        "Due to our computational resources, we're going with DistilBERT\n"
      ],
      "metadata": {
        "id": "zizMwooH20lp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1k0ogLKp027N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label_mapping))"
      ],
      "metadata": {
        "id": "EPaea8dL2CSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EmailDataset(Dataset):\n",
        "  def __init__(self, emails, tokenizer, label_mapping):\n",
        "    self.emails = emails\n",
        "    self.tokenizer = tokenizer\n",
        "    self.label_mapping = label_mapping\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.emails)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    email = self.emails[idx]\n",
        "    text = email[\"cleaned_body\"]\n",
        "    label = email[\"numeric_label\"]\n",
        "\n",
        "    encoded_input = self.tokenizer(text, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "    #return{**encoded_input, \"labels\": torch.tensor(label, dtype=torch.long)}\n",
        "    return {\"input_ids\": encoded_input[\"input_ids\"].squeeze(0), \"attention_mask\": encoded_input[\"attention_mask\"].squeeze(0), \"labels\": torch.tensor(label, dtype=torch.long)}\n",
        "\n"
      ],
      "metadata": {
        "id": "x4OZoAHk9RSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CLb3UF5pKGAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare dataset\n",
        "dataset = EmailDataset(emails, tokenizer, label_mapping)  # Create the dataset\n",
        "\n",
        "# Split into train and validation sets\n",
        "train_dataset, val_dataset = train_test_split(dataset, test_size=0.3, random_state=42)\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./email_model\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=3e-5, #try 3e-5 (slightly higher) or 1e-5 (slightly lower).\n",
        "    per_device_train_batch_size=8, #if run out of memory, do 8 instead\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=5, #or 5. try 5 later\n",
        "    weight_decay=0.01, #can increase to 0.05\n",
        "    fp16=True,\n",
        "    report_to=\"none\",  # Disable wandb logging as I don't want to use it\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_strategy='steps',       # log for every global_step\n",
        "    logging_steps=10\n",
        ")\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,  # Train data\n",
        "    eval_dataset=val_dataset  # Validation data\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "#trainer.train(resume_from_checkpoint=True)\n"
      ],
      "metadata": {
        "id": "W0sSzoZQJi-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "khKoZxjfMfjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G_75m9d5Mhqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluate model\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
        "\n",
        "trainer.compute_metrics = compute_metrics\n",
        "trainer.evaluate()\n"
      ],
      "metadata": {
        "id": "4gTf6SwdF5Tx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test predictions\n",
        "def classify_email(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "    outputs = model(**inputs)\n",
        "    predicted_label = outputs.logits.argmax(-1).item()\n",
        "    return predicted_label\n",
        "\n",
        "new_email = \"you just got admission into the university, congratulations!\"\n",
        "print(classify_email(new_email))  # See predicted label\n"
      ],
      "metadata": {
        "id": "9Sjc5dxRF62D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "predictions, labels, _ = trainer.predict(val_dataset)\n",
        "preds = np.argmax(predictions, axis=1)\n",
        "print(classification_report(labels, preds))\n"
      ],
      "metadata": {
        "id": "61mtQpkMh5u7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_mapping = {\n",
        "    \"Shopping\": 0,\n",
        "    \"Updates\": 1,\n",
        "    \"School\": 2,\n",
        "    \"We could have\": 3,\n",
        "    \"Socials\": 4,\n",
        "    \"Forums\": 5,\n",
        "    \"Promotions\": 6,\n",
        "    \"Interviews\": 7,\n",
        "    \"Offer\": 8,\n",
        "    \"Finances\": 9,\n",
        "    \"Applications\": 10,\n",
        "    \"Other\": 11\n",
        "}"
      ],
      "metadata": {
        "id": "TwQoAMM7Q5fw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-xiXNzrBi7fV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BpdzHhRmiuXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Extractive Summarization vs. Abstractive\n",
        "Now that  classification model is trained, summarization is a separate task.\n",
        "\n",
        "- Extractive Summarization: Picks the most important sentences from the email.\n",
        "\n",
        "- Abstractive Summarization: Generates a completely new summary.\n",
        "\n",
        "can use extractive summarization because:\n",
        " - It's computationally cheaper.\n",
        " - More reliable for emails (e.g., extracting key points from long messages)."
      ],
      "metadata": {
        "id": "jioH_0bBQ5jV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mO-clRHEitTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "email_text = \"\"\"How's 2025 treating you - like a VIP\\r\\n(totally on point)? Or are you already feeling the January vibe after\\r\\na Detty December? (I'm sure you gerrit) \\r\\n\\r\\n_WELL, JANUARY MIGHT FEEL LIKE IT HAS 100 DAYS, BUT DON\\u2019T WORRY,\\r\\nWITH ME, YOU WON\\u2019T EVEN NOTICE THE CLOCK TICKING \\ud83d\\ude01\\ud83d\\ude01! _\\r\\n\\r\\nTo make it even spicier, this 2025, connect with your fellow stock\\r\\ninvestors, steal\\u2026 sorry,\\ud83e\\udd23\\ud83e\\udd23 learn from their wins and losses,\\r\\nand drop your own market gist like the investment guru you are. \\r\\n\\r\\n_LET\\u2019S MAKE THOSE NAIRA (AND DOLLARS) WORK HARDER THAN WE DO!\\u00a0_ \\r\\n\\r\\nLet\\u2019s dive right into the business of today!\\r\\n\\r\\n \\t\\t 20 Years?! Quantum Computing's Shocking Truth \\r\\n\\r\\nLast year, quantum computing stocks were a thing but this new year,\\r\\nsomeone But this year? Someone just slammed the brakes on the train,\\r\\nand let\\u2019s just say\\u2026_IT WASN\\u2019T CUTE. \\ud83d\\ude2c_\\r\\n\\r\\nBefore I continue with the gist lemme tell you a little bit about the\\r\\nQuantum computing stocks. Basically, they are shares of companies\\r\\ndeveloping and investing in quantum computing technology. This\\r\\nemerging field uses quantum-mechanical phenomena to perform\\r\\ncalculations exponentially faster than classical computers. Companies\\r\\nlike Rigetti Computing ($RGTI), IonQ ($IONQ), and D-Wave Quantum\\r\\n($QBTS) are leading players in this space, attracting investors eager\\r\\nto capitalize on the potential breakthroughs.\\r\\n\\r\\n_BUT THEN THIS WEEK, JENSEN HUANG HAPPENED!!_ \\r\\n\\r\\nYep! Tech legend and Nvidia CEO Jensen Huang decided to casually pop\\r\\nthe bubble, telling analysts that \\u201cvery useful quantum computers\\u201d\\r\\nare still 20 years away. Yes, TWENTY. Not five. Not ten. Twenty\\r\\nyears!!!!. \\ud83e\\udee0 \\r\\n\\r\\nThe Street, of course, did not like the sound of that and of course\\r\\nyou know what follows next\\u2026 _A DIP!!!_ Rigetti Computing saw its\\r\\nstock drop by 45%, IonQ fell off the cliff by 39%, D-Wave Quantum went\\r\\ndown by 36%, and Quantum Computing took a hit at 43%.\\r\\n\\r\\nWhat a heartbreak.\\ud83d\\udc94 \\r\\n\\r\\n(... rest of your email content ...)\"\"\"\n",
        "\n",
        "# ... (your summarization code) ...\n",
        "\n",
        "parser = PlaintextParser.from_string(email_text, Tokenizer(\"english\")) # Pass email_text here\n",
        "# ... (rest of your summarization code) ..."
      ],
      "metadata": {
        "id": "ElqmRFI4l15s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a_text= \"\"\" Meta came with $800 million in hand, thinking it could do what it always does: buy a South Korean AI chip startup, absorb it, and move on. But guess what? The company straight-up said, ‚ÄúNah, we‚Äôre good.\" Who rejects $800 million just like that?!\n",
        "\n",
        "Whoa! FuriosaAI didn‚Äôt even flinch. Not a ‚Äúlet‚Äôs negotiate‚Äù or ‚Äúmake it $1 billion, and we‚Äôll think about it.‚Äù\n",
        "\n",
        "Uncle Mark got a cold, hard NO. I‚Äôm not saying I enjoy billionaire heartbreak, but this one? Took me OUT ü§£ü§£. Because‚Ä¶ when was the last time Uncle Mark didn‚Äôt get what he wanted? This man bought Instagram, snatched WhatsApp before anyone could blink and casually picked up Oculus. But FuriosaAI? Oh! They rejected his $800M. Ooops!\n",
        "\n",
        "FuriosaAI is a small but mighty AI chip company, and Meta wanted them to boost its AI game. But these guys are playing the long game. They‚Äôd rather build their empire than become another Meta-owned brand. They‚Äôre not here to be part of Uncle Mark‚Äôs shopping cart. They want full control over their future.\n",
        "\n",
        "Meanwhile, Uncle Mark is still on his AI buy-a-thon, pouring $65 billion into AI because he‚Äôs tired of depending on Nvidia. But this particular deal? It‚Äôs Dead on Arrival.\n",
        "\n",
        "FuriosaAI is choosing the long road, betting on itself instead of taking an easy exit. Dollar Tree Just Sold a Whole Business for $1B‚Ä¶ After Buying It for $9B\n",
        "You know Dollar Tree, right? That big American discount store where everything is supposed to be cheap? Well, they just sold off their Family Dollar chain for $1 billion after buying it for $9 billion back in 2015.\n",
        "\n",
        "Imagine buying something for 9,000 naira and selling it years later for 1,000 naira. Omo, this one is paining me on their behalf. A whole $8 billion loss. But funny enough, investors are actually CELEBRATING..Why? Because Family Dollar was that one friend who keeps borrowing money but never pays it back. (You know the type. If you don‚Äôt, you might BE the type. üëÄ)\n",
        "Family Dollar wasn‚Äôt just struggling; it was fighting for its life against big players like Walmart, Amazon, Shein, and Temu.\n",
        "\n",
        "Inflation also made things worse because people stopped spending on things like home decor and clothes, which are exactly the products Family Dollar needed to make real money.\n",
        "\n",
        "\n",
        "Dollar Tree had been trying to offload this liability for nearly a year, and now they‚Äôve finally found some private investors to take it off their hands.\n",
        "\n",
        "The moment Dollar Tree cut off the dead weight, its stock price jumped 6% like a toxic ex finally leaving the group chat. But that‚Äôs not the end of their troubles... They still have wahala ahead. Yes!\n",
        "\n",
        "With rising costs from global supply chain issues and new trade policies making imports more expensive, Dollar Tree is about to start bleeding an extra $20 million per month in costs, which means it's going to cost them $20 million per month to do business. üëÄ\n",
        "Trump‚Äôs Tariff Saga Continues Another day, another tariff! This time, foreign automakers in the US are in the hot seat!\n",
        "\n",
        "Yep, Trump just announced a 25% tariff on foreign-made cars and auto parts, and you know what? This is just the beginning because on April 2 (the new date for Liberation Day), more tariffs will hit imported goods from major U.S. trading partners.\n",
        "\n",
        "And no, this isn‚Äôt some April Fool‚Äôs prank. Trump deliberately set the date for April 2 to avoid any confusion. Companies importing vehicles from Mexico and Canada under the USMCA (United States-Mexico-Canada Agreement) won‚Äôt immediately face the 25% tariff that Trump is imposing on foreign-made cars and auto parts.\n",
        "\n",
        "Instead, they will receive \"special consideration\" (likely exemptions or delays) until the U.S. Commerce Department creates a formal process for applying these tariffs.\n",
        "\n",
        "This move is part of Trump's strategy to push companies to manufacture more products within the U.S. instead of relying on imports. This announcement isn‚Äôt sitting so well with automakers across all nations, and some are saying this could backfire, making cars more expensive and leading to job losses instead of job creation.\n",
        "\n",
        "What do you think?? I would like to hear your thoughts on Trove Social.\n",
        "Waymo‚Äôs Next Stop is Washington, D.C. Waymo has been rolling out its robotaxis in different parts of the US, and now they‚Äôre bringing their robot taxis to Washington, D.C., America‚Äôs capital city.\n",
        "\n",
        "But before you start dreaming of hopping into a car with no driver, calm down first.\n",
        " \"\"\""
      ],
      "metadata": {
        "id": "SaOulpaBks0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text= \"\"\" Meta came with $800 million in hand, thinking it could do what it always does: buy a South Korean AI chip startup, absorb it, and move on. But guess\n",
        "what? The company straight-up said, ‚ÄúNah, we‚Äôre good.\" Who rejects $800 million just like that?!  Whoa! FuriosaAI didn‚Äôt even flinch. Not a ‚Äúlet‚Äôs negotiate‚Äù or\n",
        "‚Äúmake it $1 billion, and we‚Äôll think about it.‚Äù Uncle Mark got a cold, hard NO. I‚Äôm not saying I enjoy billionaire heartbreak, but this one? Took me OUT.\n",
        "Because‚Ä¶ when was the last time Uncle Mark didn‚Äôt get what he wanted? This man bought Instagram, snatched WhatsApp before anyone could blink and casually picked\n",
        "up Oculus. But FuriosaAI? Oh! They rejected his $800M. Ooops! FuriosaAI is a small but mighty AI chip company, and Meta wanted them to boost its AI game. But\n",
        "these guys are playing the long game. They‚Äôd rather build their empire than become another Meta-owned brand. They‚Äôre not here to be part of Uncle Mark‚Äôs shopping\n",
        "cart. They want full control over their future. Meanwhile, Uncle Mark is still on his AI buy-a-thon, pouring $65 billion into AI because he‚Äôs tired of\n",
        "depending on Nvidia. But this particular deal? It‚Äôs Dead on Arrival. FuriosaAI is choosing the long road, betting on itself instead of taking an easy exit.\n",
        "Dollar Tree Just Sold a Whole Business for $1B‚Ä¶ After Buying It for $9B You know Dollar Tree, right? That big American discount store where everything is\n",
        "supposed to be cheap? Well, they just sold off their Family Dollar chain for $1 billion after buying it for $9 billion back in 2015.\n",
        "\n",
        "Imagine buying something for 9,000 naira and selling it years later for 1,000 naira. Omo, this one is paining me on their behalf. A whole $8 billion loss. But\n",
        "funny enough, investors are actually CELEBRATING..Why? Because Family Dollar was that one friend who keeps borrowing money but never pays it back. (You know the\n",
        "type. If you don‚Äôt, you might BE the type.) Family Dollar wasn‚Äôt just struggling; it was fighting for its life against big players like Walmart, Amazon, Shein,\n",
        "and Temu.\n",
        "\n",
        "Inflation also made things worse because people stopped spending on things like home decor and clothes, which are exactly the products Family Dollar needed to\n",
        "make real money. Dollar Tree had been trying to offload this liability for nearly a year, and now they‚Äôve finally found some private investors to take it off\n",
        "their hands.  The moment Dollar Tree cut off the dead weight, its stock price jumped 6% like a toxic ex finally leaving the group chat. But that‚Äôs not the end\n",
        "of their troubles... They still have wahala ahead. Yes! With rising costs from global supply chain issues and new trade policies making imports more expensive,\n",
        "Dollar Tree is about to start bleeding an extra $20 million per month in costs, which means it's going to cost them $20 million per month to do business.\n",
        "Trump‚Äôs Tariff Saga Continues Another day, another tariff! This time, foreign automakers in the US are in the hot seat!  Yep, Trump just announced a 25% tariff\n",
        "on foreign-made cars and auto parts, and you know what? This is just the beginning because on April 2 (the new date for Liberation Day), more tariffs will hit\n",
        "imported goods from major U.S. trading partners. And no, this isn‚Äôt some April Fool‚Äôs prank. Trump deliberately set the date for April 2 to avoid any confusion.\n",
        "Companies importing vehicles from Mexico and Canada under the USMCA (United States-Mexico-Canada Agreement) won‚Äôt immediately face the 25% tariff that Trump is\n",
        "imposing on foreign-made cars and auto parts. Instead, they will receive \"special consideration\" (likely exemptions or delays) until the U.S. Commerce\n",
        "Department creates a formal process for applying these tariffs. This move is part of Trump's strategy to push companies to manufacture more products within the\n",
        "U.S. instead of relying on imports. This announcement isn‚Äôt sitting so well with automakers across all nations, and some are saying this could backfire, making\n",
        "cars more expensive and leading to job losses instead of job creation. What do you think?? I would like to hear your thoughts on Trove Social.\n",
        "Waymo‚Äôs Next Stop is Washington, D.C. Waymo has been rolling out its robotaxis in different parts of the US, and now they‚Äôre bringing their robot taxis to\n",
        "Washington, D.C., America‚Äôs capital city. But before you start dreaming of hopping into a car with no driver, calm down first. For now, the taxis will still\n",
        "have a safety driver sitting behind the wheel, just in case something goes wrong. The government has rules that don‚Äôt allow fully driverless taxis yet, so\n",
        "they‚Äôre still in the testing phase, mapping the roads and learning the city's layout.  If all goes well, by next year, people in Washington will be able to order\n",
        "a driverless taxi through the Waymo app; no driver to speak with, no steering, just pure tech magic. Waymo is already operating in Phoenix, Los Angeles, San\n",
        "Francisco, and Austin, where they even partnered with Uber. Now, they‚Äôre expanding to Miami and Washington next year, and they also plan to launch in Atlanta\n",
        "later this year.\n",
        "\n",
        " \"\"\""
      ],
      "metadata": {
        "id": "Yc0YyVzIv-s3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#extractive summarization\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lsa import LsaSummarizer\n",
        "\n",
        "parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
        "summarizer = LsaSummarizer()\n",
        "summary = summarizer(parser.document, 3)  # Extract top 3 sentences\n",
        "\n",
        "for sent in summary:\n",
        "    print(sent)\n",
        "#print(\" \".join(str(sent) for sent in summary))\n"
      ],
      "metadata": {
        "id": "GUYrkskFi1_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SUMMARIZATION WORKS 60% WELL\n",
        "\n",
        "CLASSIFICATION-- NOT SO GOOD"
      ],
      "metadata": {
        "id": "BpQSLgzEl_bm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#abstractive summarization\n",
        "\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "# Load T5-small (lightweight version for Colab)\n",
        "model_name = \"t5-small\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "def summarize_email(email_text, max_length=50):\n",
        "    # Prepend \"summarize: \" for T5\n",
        "    input_text = \"summarize: \" + email_text\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
        "\n",
        "    # Generate summary\n",
        "    summary_ids = model.generate(inputs.input_ids, max_length=max_length, min_length=10, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n"
      ],
      "metadata": {
        "id": "MCTpbyStvgor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Example email\n",
        "email_text = \"\"\"\n",
        "The error LookupError: Resource punkt_tab not found. indicates that the necessary NLTK data package punkt_tab is not downloaded. This package provides tokenizers (tools that break text into sentences and words) needed by the sumy library for text summarization.\n",
        "\n",
        "The sumy library relies on NLTK's punkt tokenizer to segment the text into sentences. If punkt isn't available, it can't process the text correctly. The code attempts to load this tokenizer, but since the data isn't present, it throws a LookupError.\n",
        "\"\"\"\n",
        "print(summarize_email(email_text))\n"
      ],
      "metadata": {
        "id": "7T2m5fONvprJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OMPLEMENTING FEW SHOT LEARNING FOR EMAIL CLASSIFICATION\n",
        "\n",
        "Few-shot learning allows your model to quickly adapt to new email categories with minimal labeled examples. 3 main categories:\n",
        "\n",
        "- Fine-Tune with Additional Data (Standard Approach). If you get a new category (e.g., \"Medical Appointments\"), fine-tune the model again with a few labeled examples\n",
        "\n",
        "- Use Prompt-Based Learning (For Transformers like GPT, T5). Instead of retraining, some models like T5 and GPT-based models can learn new categories dynamically from a prompt.\n",
        "\n",
        "- Use Embeddings + Nearest Neighbor for New Classes. Instead of retraining, store embeddings of past emails and classify new emails by comparing similarity."
      ],
      "metadata": {
        "id": "pPyGVdMqyb6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#embeddings use case\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # Small & fast\n",
        "\n",
        "# Existing category embeddings\n",
        "category_texts = {\n",
        "    \"Urgent\": \"Immediate action needed!\",\n",
        "    \"Job\": \"Interview scheduled.\",\n",
        "    \"Shopping\": \"Your order is confirmed.\"\n",
        "}\n",
        "category_embeddings = {cat: model.encode(text) for cat, text in category_texts.items()}\n",
        "\n",
        "# New email\n",
        "new_email = \"order should arrive by Friday.\"\n",
        "new_email_embedding = model.encode(new_email)\n",
        "\n",
        "# Find the closest category\n",
        "similarities = {cat: np.dot(new_email_embedding, emb) for cat, emb in category_embeddings.items()}\n",
        "predicted_category = max(similarities, key=similarities.get)\n",
        "print(predicted_category)\n"
      ],
      "metadata": {
        "id": "cVQlOffDv7R1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prompt based learning use case\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "model_name = \"t5-small\"\n",
        "t5_model = T5ForConditionalGeneration.from_pretrained(model_name) # Renamed to avoid conflict\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "input_text = \"Classify this email: 'Your appointment is on Friday.' \\n Categories: Urgent, Job, Shopping, Medical\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "outputs = t5_model.generate(inputs.input_ids)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "beo_DAlzx4AY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "22VgLVzoy9lJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}