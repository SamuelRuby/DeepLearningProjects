{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ETtu9CvVMDR"
      },
      "source": [
        "<h1>Chapter 10 - Creating Text Embedding Models</h1>\n",
        "<i>Exploring methods for both training and fine-tuning embedding models.</i>\n",
        "\n",
        "\n",
        "\n",
        "This notebook is for Chapter 10 of the [Hands-On Large Language Models](https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961) book by [Jay Alammar](https://www.linkedin.com/in/jalammar) and [Maarten Grootendorst](https://www.linkedin.com/in/mgrootendorst/).\n",
        "\n",
        "---\n",
        "\n",
        "<a href=\"https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961\">\n",
        "<img src=\"https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/images/book_cover.png\" width=\"350\"/></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8N-Qzbh_2QUp"
      },
      "source": [
        "### [OPTIONAL] - Installing Packages on <img src=\"https://colab.google/static/images/icons/colab.png\" width=100>\n",
        "\n",
        "If you are viewing this notebook on Google Colab (or any other cloud vendor), you need to **uncomment and run** the following codeblock to install the dependencies for this chapter:\n",
        "\n",
        "---\n",
        "\n",
        "💡 **NOTE**: We will want to use a GPU to run the examples in this notebook. In Google Colab, go to\n",
        "**Runtime > Change runtime type > Hardware accelerator > GPU > GPU type > T4**.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_xmlp-Ff2QUr"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -q accelerate>=0.27.2 peft>=0.9.0 bitsandbytes>=0.43.0 transformers>=4.38.2 trl>=0.7.11 sentencepiece>=0.1.99\n",
        "!pip install -q sentence-transformers>=3.0.0 mteb>=1.1.2 datasets>=2.18.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Disable Weights & Biases logging for this session\n",
        "%env WANDB_DISABLED=true"
      ],
      "metadata": {
        "id": "OuGhkFRFISiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UrKluX5YNmu"
      },
      "source": [
        "# Creating an Embedding Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywsyZzm5VSER"
      },
      "source": [
        "## **Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ahk0SJDKVy6F"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load MNLI dataset from GLUE\n",
        "# 0 = entailment, 1 = neutral, 2 = contradiction\n",
        "train_dataset = load_dataset(\"glue\", \"mnli\", split=\"train\").select(range(50_000))\n",
        "train_dataset = train_dataset.remove_columns(\"idx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-BHO4-qwMDO",
        "outputId": "89cb52a6-50cd-4279-980d-919b44de7410"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'premise': 'One of our number will carry out your instructions minutely.',\n",
              " 'hypothesis': 'A member of my team will execute your orders with immense precision.',\n",
              " 'label': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "train_dataset[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wO23cXLXeFU"
      },
      "source": [
        "## **Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4qLaPR6nrqC"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Use a base model\n",
        "embedding_model = SentenceTransformer('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAiL21AuYKVI"
      },
      "source": [
        "## **Loss Function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OgmtKckBXiK9"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import losses\n",
        "\n",
        "# Define the loss function. In soft-max loss, we will also need to explicitly set the number of labels.\n",
        "train_loss = losses.SoftmaxLoss(\n",
        "    model=embedding_model,\n",
        "    sentence_embedding_dimension=embedding_model.get_sentence_embedding_dimension(),\n",
        "    num_labels=3\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tH0efspwlOX2"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8ZsoY0AretV"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
        "\n",
        "# Create an embedding similarity evaluator for stsb\n",
        "val_sts = load_dataset('glue', 'stsb', split='validation')\n",
        "evaluator = EmbeddingSimilarityEvaluator(\n",
        "    sentences1=val_sts[\"sentence1\"],\n",
        "    sentences2=val_sts[\"sentence2\"],\n",
        "    scores=[score/5 for score in val_sts[\"label\"]],\n",
        "    main_similarity=\"cosine\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umikSmoYIP07"
      },
      "source": [
        "## **Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8uAAhNs0ocoV"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers.training_args import SentenceTransformerTrainingArguments\n",
        "\n",
        "# Define the training arguments\n",
        "args = SentenceTransformerTrainingArguments(\n",
        "    output_dir=\"base_embedding_model\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    warmup_steps=100,\n",
        "    fp16=True,\n",
        "    eval_steps=100,\n",
        "    logging_steps=100,\n",
        "    report_to=\"none\",\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKA_L39FpAoM"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers.trainer import SentenceTransformerTrainer\n",
        "\n",
        "# Train embedding model\n",
        "trainer = SentenceTransformerTrainer(\n",
        "    model=embedding_model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    loss=train_loss,\n",
        "    evaluator=evaluator\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate our trained model\n",
        "evaluator(embedding_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4QWzhZ32dmp",
        "outputId": "35b2891f-4a11-4b6e-c7a6-e684e50b2ae9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'pearson_cosine': 0.5887710683357128, 'spearman_cosine': 0.6549722847435776}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fAtrPQvo2hGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NA16lEaseOq",
        "outputId": "b5d86d22-480d-4c5c-efd1-8e80c03dab4d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'pearson_cosine': 0.3710938716460552,\n",
              " 'spearman_cosine': 0.45148122260403883,\n",
              " 'pearson_manhattan': 0.4037396904694362,\n",
              " 'spearman_manhattan': 0.4396893995197567,\n",
              " 'pearson_euclidean': 0.390788259199341,\n",
              " 'spearman_euclidean': 0.43444104358464286,\n",
              " 'pearson_dot': 0.3392927926047231,\n",
              " 'spearman_dot': 0.3530708415227247,\n",
              " 'pearson_max': 0.4037396904694362,\n",
              " 'spearman_max': 0.45148122260403883}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#old\n",
        "# Evaluate our trained model\n",
        "#evaluator(embedding_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9xjkvCWwrp_"
      },
      "source": [
        "# MTEB"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MTEB evaluation using manual task evaluation (working solution)\n",
        "from mteb.tasks import Banking77Classification\n",
        "\n",
        "# Evaluate the model on Banking77Classification task\n",
        "task = Banking77Classification()\n",
        "results = task.evaluate(embedding_model)\n",
        "print(\"Banking77Classification results:\", results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlvrMyfC7GQq",
        "outputId": "f5b1d15e-0339-47bf-c1e9-6c49ee5b36ce"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Banking77Classification results: {'default': {'accuracy': np.float64(0.6244155844155844), 'f1': np.float64(0.6232381004044815), 'f1_weighted': np.float64(0.6232381004044815), 'scores_per_experiment': [{'accuracy': 0.6211038961038962, 'f1': 0.6201562010121457, 'f1_weighted': 0.6201562010121456}, {'accuracy': 0.6253246753246753, 'f1': 0.6251717089893252, 'f1_weighted': 0.6251717089893253}, {'accuracy': 0.6298701298701299, 'f1': 0.628180609041159, 'f1_weighted': 0.6281806090411591}, {'accuracy': 0.6373376623376623, 'f1': 0.6388685152989537, 'f1_weighted': 0.6388685152989537}, {'accuracy': 0.6220779220779221, 'f1': 0.6204426128748353, 'f1_weighted': 0.6204426128748353}, {'accuracy': 0.6272727272727273, 'f1': 0.6252478343386411, 'f1_weighted': 0.6252478343386411}, {'accuracy': 0.6256493506493507, 'f1': 0.6234371153403124, 'f1_weighted': 0.6234371153403122}, {'accuracy': 0.6133116883116884, 'f1': 0.6119995756865183, 'f1_weighted': 0.6119995756865184}, {'accuracy': 0.6262987012987012, 'f1': 0.6249370252048678, 'f1_weighted': 0.6249370252048679}, {'accuracy': 0.615909090909091, 'f1': 0.613939806258056, 'f1_weighted': 0.6139398062580559}], 'main_score': np.float64(0.6244155844155844)}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Solution 2:\n",
        "# # Create a wrapper class to fix the metadata issue\n",
        "# class MTEBCompatibleModel:\n",
        "#     def __init__(self, model):\n",
        "#         self.model = model\n",
        "\n",
        "#     def encode(self, sentences, **kwargs):\n",
        "#         return self.model.encode(sentences, **kwargs)\n",
        "\n",
        "#     def __getattr__(self, name):\n",
        "#         return getattr(self.model, name)\n",
        "\n",
        "# # Wrap the model\n",
        "# wrapped_model = MTEBCompatibleModel(embedding_model)\n",
        "\n",
        "# # Set the required metadata\n",
        "# wrapped_model.mteb_model_meta = {\n",
        "#     \"name\": \"custom-bert-base-uncased\",\n",
        "#     \"languages\": [\"en\"],  # This should be a list of language codes\n",
        "#     \"open_source\": True,\n",
        "#     \"revision\": \"1.0.0\",\n",
        "#     \"release_date\": \"2024-01-01\"\n",
        "# }\n",
        "\n",
        "# # Calculate results\n",
        "# try:\n",
        "#     results = evaluation.run(wrapped_model)\n",
        "#     print(\"Results:\", results)\n",
        "# except Exception as e:\n",
        "#     print(f\"Error with wrapped model: {e}\")\n",
        "\n",
        "#     # Solution 3: If above doesn't work, try direct model evaluation\n",
        "#     try:\n",
        "#         # Use the original model directly with proper task specification\n",
        "#         tasks = mteb.get_tasks(tasks=[\"Banking77Classification\"])\n",
        "#         evaluation = MTEB(tasks=tasks)\n",
        "#         results = evaluation.run(embedding_model)\n",
        "#         print(\"Results:\", results)\n",
        "#     except Exception as e2:\n",
        "#         print(f\"Error with direct model: {e2}\")\n",
        "\n",
        "#         # Solution 4: Manual evaluation as fallback\n",
        "#         print(\"Falling back to manual evaluation...\")\n",
        "#         from mteb.tasks import Banking77Classification\n",
        "#         task = Banking77Classification()\n",
        "#         results = task.evaluate(embedding_model)\n",
        "#         print(\"Manual evaluation results:\", results)"
      ],
      "metadata": {
        "id": "3rmTflwR8ytx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56V2ma89uJwN"
      },
      "source": [
        "⚠️ **VRAM Clean-up** - You will need to run the code below to partially empty the VRAM (GPU RAM). If that does not work, it is advised to restart the notebook instead. You can check the resources on the right-hand side (if you are using Google Colab) to check whether the used VRAM is indeed low. You can also run `!nivia-smi` to check current usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3LX1G0_4QCv"
      },
      "outputs": [],
      "source": [
        "# # Empty and delete trainer/model\n",
        "# trainer.accelerator.clear()\n",
        "# del trainer, embedding_model\n",
        "\n",
        "# # Garbage collection and empty cache\n",
        "# import gc\n",
        "# import torch\n",
        "\n",
        "# gc.collect()\n",
        "# torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "6d0GcY8cnNs4"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import torch\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYnRRSDN06eB"
      },
      "source": [
        "# Loss Fuctions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuSCWbFO7RRM"
      },
      "source": [
        "⚠️ **VRAM Clean-up**\n",
        "* `Restart` the notebook in order to clean-up memory if you move on to the next training example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tq8Yb6IB2LFI"
      },
      "source": [
        "## Cosine Similarity Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "qEmnjQQPuszQ"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset, load_dataset\n",
        "\n",
        "# Load MNLI dataset from GLUE\n",
        "# 0 = entailment, 1 = neutral, 2 = contradiction\n",
        "train_dataset = load_dataset(\"glue\", \"mnli\", split=\"train\").select(range(50_000))\n",
        "train_dataset = train_dataset.remove_columns(\"idx\")\n",
        "\n",
        "# (neutral/contradiction)=0 and (entailment)=1\n",
        "mapping = {2: 0, 1: 0, 0:1}\n",
        "train_dataset = Dataset.from_dict({\n",
        "    \"sentence1\": train_dataset[\"premise\"],\n",
        "    \"sentence2\": train_dataset[\"hypothesis\"],\n",
        "    \"label\": [float(mapping[label]) for label in train_dataset[\"label\"]]\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "np5bMwgO5y8g"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
        "\n",
        "# Create an embedding similarity evaluator for stsb\n",
        "val_sts = load_dataset('glue', 'stsb', split='validation')\n",
        "evaluator = EmbeddingSimilarityEvaluator(\n",
        "    sentences1=val_sts[\"sentence1\"],\n",
        "    sentences2=val_sts[\"sentence2\"],\n",
        "    scores=[score/5 for score in val_sts[\"label\"]],\n",
        "    main_similarity=\"cosine\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ikky866vdseY"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import losses, SentenceTransformer\n",
        "from sentence_transformers.trainer import SentenceTransformerTrainer\n",
        "from sentence_transformers.training_args import SentenceTransformerTrainingArguments\n",
        "\n",
        "# Define model\n",
        "embedding_model = SentenceTransformer('bert-base-uncased')\n",
        "\n",
        "# Loss function\n",
        "train_loss = losses.CosineSimilarityLoss(model=embedding_model)\n",
        "\n",
        "# Define the training arguments\n",
        "args = SentenceTransformerTrainingArguments(\n",
        "    output_dir=\"cosineloss_embedding_model\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    warmup_steps=100,\n",
        "    fp16=True,\n",
        "    eval_steps=100,\n",
        "    logging_steps=100,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "# Train model\n",
        "trainer = SentenceTransformerTrainer(\n",
        "    model=embedding_model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    loss=train_loss,\n",
        "    evaluator=evaluator\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate our trained model\n",
        "evaluator(embedding_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXK_v2aHC-mn",
        "outputId": "de7e1240-7f1e-4cfc-f622-928e6b7a677e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'pearson_cosine': 0.7291690635526045, 'spearman_cosine': 0.7318555228549017}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnGpHqy46YS3"
      },
      "source": [
        "⚠️ **VRAM Clean-up**\n",
        "* `Restart` the notebook in order to clean-up memory if you move on to the next training example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Xvps7UpznPD4"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import torch\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yh0toLx2Ni7"
      },
      "source": [
        "## Multiple Negatives Ranking Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzToWFH0vZzz",
        "outputId": "dadc6f34-52fe-45a3-d574-9081033d08ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "16875it [00:01, 14236.32it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16875"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "import random\n",
        "from tqdm import tqdm\n",
        "from datasets import Dataset, load_dataset\n",
        "\n",
        "# # Load MNLI dataset from GLUE\n",
        "mnli = load_dataset(\"glue\", \"mnli\", split=\"train\").select(range(50_000))\n",
        "mnli = mnli.remove_columns(\"idx\")\n",
        "mnli = mnli.filter(lambda x: True if x['label'] == 0 else False)\n",
        "\n",
        "# Prepare data and add a soft negative\n",
        "train_dataset = {\"anchor\": [], \"positive\": [], \"negative\": []}\n",
        "soft_negatives = mnli[\"hypothesis\"]\n",
        "random.shuffle(soft_negatives)\n",
        "for row, soft_negative in tqdm(zip(mnli, soft_negatives)):\n",
        "    train_dataset[\"anchor\"].append(row[\"premise\"])\n",
        "    train_dataset[\"positive\"].append(row[\"hypothesis\"])\n",
        "    train_dataset[\"negative\"].append(soft_negative)\n",
        "train_dataset = Dataset.from_dict(train_dataset)\n",
        "len(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "wP_s1yAB7D7I"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
        "\n",
        "# Create an embedding similarity evaluator for stsb\n",
        "val_sts = load_dataset('glue', 'stsb', split='validation')\n",
        "evaluator = EmbeddingSimilarityEvaluator(\n",
        "    sentences1=val_sts[\"sentence1\"],\n",
        "    sentences2=val_sts[\"sentence2\"],\n",
        "    scores=[score/5 for score in val_sts[\"label\"]],\n",
        "    main_similarity=\"cosine\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-Q2m0yzkRvW"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import losses, SentenceTransformer\n",
        "from sentence_transformers.trainer import SentenceTransformerTrainer\n",
        "from sentence_transformers.training_args import SentenceTransformerTrainingArguments\n",
        "\n",
        "# Define model\n",
        "embedding_model = SentenceTransformer('bert-base-uncased')\n",
        "\n",
        "# Loss function\n",
        "train_loss = losses.MultipleNegativesRankingLoss(model=embedding_model)\n",
        "\n",
        "# Define the training arguments\n",
        "args = SentenceTransformerTrainingArguments(\n",
        "    output_dir=\"mnrloss_embedding_model\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    warmup_steps=100,\n",
        "    fp16=True,\n",
        "    eval_steps=100,\n",
        "    logging_steps=100,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "# Train model\n",
        "trainer = SentenceTransformerTrainer(\n",
        "    model=embedding_model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    loss=train_loss,\n",
        "    evaluator=evaluator\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvPEvgf98uS8",
        "outputId": "5e14a67f-95ba-489b-d1dc-a8b0305803cc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'pearson_cosine': 0.8095226330438382, 'spearman_cosine': 0.8112846488441666}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "# Evaluate our trained model\n",
        "evaluator(embedding_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ND1ej1ag054E"
      },
      "source": [
        "# **Fine-tuning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqATI-1V7coM"
      },
      "source": [
        "⚠️ **VRAM Clean-up**\n",
        "* `Restart` the notebook in order to clean-up memory if you move on to the next training example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkWj0SfYnRFd"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import torch\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGxyXucEkjfw"
      },
      "source": [
        "## **Supervised**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "5GXBTm_C-IPE"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
        "\n",
        "# Load MNLI dataset from GLUE\n",
        "# 0 = entailment, 1 = neutral, 2 = contradiction\n",
        "train_dataset = load_dataset(\"glue\", \"mnli\", split=\"train\").select(range(50_000))\n",
        "train_dataset = train_dataset.remove_columns(\"idx\")\n",
        "\n",
        "# Create an embedding similarity evaluator for stsb\n",
        "val_sts = load_dataset('glue', 'stsb', split='validation')\n",
        "evaluator = EmbeddingSimilarityEvaluator(\n",
        "    sentences1=val_sts[\"sentence1\"],\n",
        "    sentences2=val_sts[\"sentence2\"],\n",
        "    scores=[score/5 for score in val_sts[\"label\"]],\n",
        "    main_similarity=\"cosine\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQ3vW2VTA9dN"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import losses, SentenceTransformer\n",
        "from sentence_transformers.trainer import SentenceTransformerTrainer\n",
        "from sentence_transformers.training_args import SentenceTransformerTrainingArguments\n",
        "\n",
        "# Define model\n",
        "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "# Loss function\n",
        "train_loss = losses.MultipleNegativesRankingLoss(model=embedding_model)\n",
        "\n",
        "# Define the training arguments\n",
        "args = SentenceTransformerTrainingArguments(\n",
        "    output_dir=\"finetuned_embedding_model\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    warmup_steps=100,\n",
        "    fp16=True,\n",
        "    eval_steps=100,\n",
        "    logging_steps=100,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "# Train model\n",
        "trainer = SentenceTransformerTrainer(\n",
        "    model=embedding_model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    loss=train_loss,\n",
        "    evaluator=evaluator\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaPJIpkS-ZrT",
        "outputId": "a3f57c05-5e69-45b3-bc86-1caef2b482bd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'pearson_cosine': 0.8495169477565929, 'spearman_cosine': 0.8488979719545471}"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "# Evaluate our trained model\n",
        "evaluator(embedding_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pHpVCwmk-XW",
        "outputId": "d31a6cf6-68e4-49da-9bdb-40fa80275106"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'pearson_cosine': 0.8696194532655239, 'spearman_cosine': 0.8671631197908374}"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "# Evaluate the pre-trained model\n",
        "original_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "evaluator(original_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ii6sIMpH7d7S"
      },
      "source": [
        "⚠️ **VRAM Clean-up**\n",
        "* `Restart` the notebook in order to clean-up memory if you move on to the next training example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGCTfC-unSL1"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import torch\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvCPXCSZkkxm"
      },
      "source": [
        "## **Augmented SBERT**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtoEArJElrZh"
      },
      "source": [
        "**Step 1:** Fine-tune a cross-encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJhEDAeeyhPD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset, Dataset\n",
        "from sentence_transformers import InputExample\n",
        "from sentence_transformers.datasets import NoDuplicatesDataLoader\n",
        "\n",
        "# Prepare a small set of 10000 documents for the cross-encoder\n",
        "dataset = load_dataset(\"glue\", \"mnli\", split=\"train\").select(range(10_000))\n",
        "mapping = {2: 0, 1: 0, 0:1}\n",
        "\n",
        "# Data Loader\n",
        "gold_examples = [\n",
        "    InputExample(texts=[row[\"premise\"], row[\"hypothesis\"]], label=mapping[row[\"label\"]])\n",
        "    for row in tqdm(dataset)\n",
        "]\n",
        "gold_dataloader = NoDuplicatesDataLoader(gold_examples, batch_size=32)\n",
        "\n",
        "# Pandas DataFrame for easier data handling\n",
        "gold = pd.DataFrame(\n",
        "    {\n",
        "    'sentence1': dataset['premise'],\n",
        "    'sentence2': dataset['hypothesis'],\n",
        "    'label': [mapping[label] for label in dataset['label']]\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_MHAJzl2H6Z"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers.cross_encoder import CrossEncoder\n",
        "\n",
        "# Train a cross-encoder on the gold dataset\n",
        "cross_encoder = CrossEncoder('bert-base-uncased', num_labels=2)\n",
        "cross_encoder.fit(\n",
        "    train_dataloader=gold_dataloader,\n",
        "    epochs=1,\n",
        "    show_progress_bar=True,\n",
        "    warmup_steps=100,\n",
        "    use_amp=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0OcVG6WmMOJ"
      },
      "source": [
        "**Step 2:** Create new sentence pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "fgx8N8a8kVrZ"
      },
      "outputs": [],
      "source": [
        "# Prepare the silver dataset by predicting labels with the cross-encoder\n",
        "silver = load_dataset(\"glue\", \"mnli\", split=\"train\").select(range(10_000, 50_000))\n",
        "pairs = list(zip(silver['premise'], silver['hypothesis']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcG7cDG5qrwX"
      },
      "source": [
        "**Step 3:** Label new sentence pairs with the fine-tuned cross-encoder (silver dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9Yuhzxq2NMj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Label the sentence pairs using our fine-tuned cross-encoder\n",
        "output = cross_encoder.predict(pairs, apply_softmax=True, show_progress_bar=True)\n",
        "silver = pd.DataFrame(\n",
        "    {\n",
        "        \"sentence1\": silver[\"premise\"],\n",
        "        \"sentence2\": silver[\"hypothesis\"],\n",
        "        \"label\": np.argmax(output, axis=1)\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9Jd-Kssqzk_"
      },
      "source": [
        "**Step 4:** Train a bi-encoder (SBERT) on the extended dataset (gold + silver dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "fp09qxMhzagi"
      },
      "outputs": [],
      "source": [
        "# Combine gold + silver\n",
        "data = pd.concat([gold, silver], ignore_index=True, axis=0)\n",
        "data = data.drop_duplicates(subset=['sentence1', 'sentence2'], keep=\"first\")\n",
        "train_dataset = Dataset.from_pandas(data, preserve_index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "S-6RW_wOAOwO"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
        "\n",
        "# Create an embedding similarity evaluator for stsb\n",
        "val_sts = load_dataset('glue', 'stsb', split='validation')\n",
        "evaluator = EmbeddingSimilarityEvaluator(\n",
        "    sentences1=val_sts[\"sentence1\"],\n",
        "    sentences2=val_sts[\"sentence2\"],\n",
        "    scores=[score/5 for score in val_sts[\"label\"]],\n",
        "    main_similarity=\"cosine\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MK1KybOI_uIY"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import losses, SentenceTransformer\n",
        "from sentence_transformers.trainer import SentenceTransformerTrainer\n",
        "from sentence_transformers.training_args import SentenceTransformerTrainingArguments\n",
        "\n",
        "# Define model\n",
        "embedding_model = SentenceTransformer('bert-base-uncased')\n",
        "\n",
        "# Loss function\n",
        "train_loss = losses.CosineSimilarityLoss(model=embedding_model)\n",
        "\n",
        "# Define the training arguments\n",
        "args = SentenceTransformerTrainingArguments(\n",
        "    output_dir=\"augmented_embedding_model\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    warmup_steps=100,\n",
        "    fp16=True,\n",
        "    eval_steps=100,\n",
        "    logging_steps=100,\n",
        ")\n",
        "\n",
        "# Train model\n",
        "trainer = SentenceTransformerTrainer(\n",
        "    model=embedding_model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    loss=train_loss,\n",
        "    evaluator=evaluator\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "9_NHjK75z58G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b8b4cd5-6f44-4bcc-d74f-9670a00cc436"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'pearson_cosine': 0.6896120069729746, 'spearman_cosine': 0.706926317038891}"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "# Evaluate our trained model\n",
        "evaluator(embedding_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwAaFBHvDcFi"
      },
      "outputs": [],
      "source": [
        "trainer.accelerator.clear()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CX6lArIH0h1A"
      },
      "source": [
        "**Step 5**: Evaluate without silver dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyPBGfxp0D_7"
      },
      "outputs": [],
      "source": [
        "# Combine gold + silver\n",
        "data = pd.concat([gold], ignore_index=True, axis=0)\n",
        "data = data.drop_duplicates(subset=['sentence1', 'sentence2'], keep=\"first\")\n",
        "train_dataset = Dataset.from_pandas(data, preserve_index=False)\n",
        "\n",
        "# Define model\n",
        "embedding_model = SentenceTransformer('bert-base-uncased')\n",
        "\n",
        "# Loss function\n",
        "train_loss = losses.CosineSimilarityLoss(model=embedding_model)\n",
        "\n",
        "# Define the training arguments\n",
        "args = SentenceTransformerTrainingArguments(\n",
        "    output_dir=\"gold_only_embedding_model\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    warmup_steps=100,\n",
        "    fp16=True,\n",
        "    eval_steps=100,\n",
        "    logging_steps=100,\n",
        ")\n",
        "\n",
        "# Train model\n",
        "trainer = SentenceTransformerTrainer(\n",
        "    model=embedding_model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    loss=train_loss,\n",
        "    evaluator=evaluator\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "6L8_5TLJ0jdK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6166f512-3b17-4993-cb59-3eeda265c22b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'pearson_cosine': 0.6195613872419589, 'spearman_cosine': 0.6466427825540182}"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "# Evaluate our trained model\n",
        "evaluator(embedding_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZujSUsu0sHU"
      },
      "source": [
        "Compared to using both the silver and gold datasets, using only the gold dataset reduces the performance of the model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVq3FSZL7gK7"
      },
      "source": [
        "⚠️ **VRAM Clean-up**\n",
        "* `Restart` the notebook in order to clean-up memory if you move on to the next training example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "6gckDRJ1nUfo"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import torch\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7RNAKVl3wmM"
      },
      "source": [
        "## **Unsupervised Learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oq_phjTb31gX"
      },
      "source": [
        "### Tranformer-based Denoising AutoEncoder (TSDAE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8yMdUf_WwErS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d19431a7-a3d7-49b4-c535-d50877df3dda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Download additional tokenizer\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruI-lOZYZt7J"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from datasets import Dataset, load_dataset\n",
        "from sentence_transformers.datasets import DenoisingAutoEncoderDataset\n",
        "\n",
        "# Create a flat list of sentences\n",
        "mnli = load_dataset(\"glue\", \"mnli\", split=\"train\").select(range(25_000))\n",
        "flat_sentences = mnli[\"premise\"] + mnli[\"hypothesis\"]\n",
        "\n",
        "# Add noise to our input data\n",
        "damaged_data = DenoisingAutoEncoderDataset(list(set(flat_sentences)))\n",
        "\n",
        "# Create dataset\n",
        "train_dataset = {\"damaged_sentence\": [], \"original_sentence\": []}\n",
        "for data in tqdm(damaged_data):\n",
        "    train_dataset[\"damaged_sentence\"].append(data.texts[0])\n",
        "    train_dataset[\"original_sentence\"].append(data.texts[1])\n",
        "train_dataset = Dataset.from_dict(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "mymxiQ9A1eQm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6075aa6d-bc3b-4828-d21a-3a163445a59e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'damaged_sentence': 'teach but',\n",
              " 'original_sentence': 'i teach but i teach for Dallas'}"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "train_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77IuQ8QIjO25"
      },
      "outputs": [],
      "source": [
        "# # Choose a different deletion ratio\n",
        "# flat_sentences = list(set(flat_sentences))\n",
        "# damaged_data = DenoisingAutoEncoderDataset(\n",
        "#     flat_sentences,\n",
        "#     noise_fn=lambda s: DenoisingAutoEncoderDataset.delete(s, del_ratio=0.6)\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "Tl6CzdwNA1tC"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
        "\n",
        "# Create an embedding similarity evaluator for stsb\n",
        "val_sts = load_dataset('glue', 'stsb', split='validation')\n",
        "evaluator = EmbeddingSimilarityEvaluator(\n",
        "    sentences1=val_sts[\"sentence1\"],\n",
        "    sentences2=val_sts[\"sentence2\"],\n",
        "    scores=[score/5 for score in val_sts[\"label\"]],\n",
        "    main_similarity=\"cosine\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "pYM298tWlacT"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import models, SentenceTransformer\n",
        "\n",
        "# Create your embedding model\n",
        "word_embedding_model = models.Transformer('bert-base-uncased')\n",
        "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), 'cls')\n",
        "embedding_model = SentenceTransformer(modules=[word_embedding_model, pooling_model])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "5RZ8tQFSlIHm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "004a005b-33f5-45c6-af3e-cde3e7946849"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.value.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "The following encoder weights were not tied to the decoder ['bert/pooler']\n"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import losses\n",
        "\n",
        "# Use the denoising auto-encoder loss\n",
        "train_loss = losses.DenoisingAutoEncoderLoss(\n",
        "    embedding_model, tie_encoder_decoder=True\n",
        ")\n",
        "train_loss.decoder = train_loss.decoder.to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYApurOS07x0"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers.trainer import SentenceTransformerTrainer\n",
        "from sentence_transformers.training_args import SentenceTransformerTrainingArguments\n",
        "\n",
        "# Define the training arguments\n",
        "args = SentenceTransformerTrainingArguments(\n",
        "    output_dir=\"tsdae_embedding_model\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    warmup_steps=100,\n",
        "    fp16=True,\n",
        "    eval_steps=100,\n",
        "    logging_steps=100,\n",
        ")\n",
        "\n",
        "# Train model\n",
        "trainer = SentenceTransformerTrainer(\n",
        "    model=embedding_model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    loss=train_loss,\n",
        "    evaluator=evaluator\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGxh6fTa7qIh"
      },
      "outputs": [],
      "source": [
        "# Evaluate our trained model\n",
        "evaluator(embedding_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAd7OKerm-w8"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import torch\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8837dfe"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}